---
domain: langfuse.com
path: /docs/integrations/langchain/example-python
url: https://langfuse.com/docs/integrations/langchain/example-python
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Setup](#setup)
    
*   [Examples](#examples)
    
*   [Sequential Chain in Langchain Expression Language (LCEL)](#sequential-chain-in-langchain-expression-language-lcel)
    
*   [Runnable methods](#runnable-methods)
    
*   [ConversationChain](#conversationchain)
    
*   [RetrievalQA](#retrievalqa)
    
*   [Agent](#agent)
    
*   [AzureOpenAI](#azureopenai)
    
*   [Sequential Chain \[Legacy\]](#sequential-chain-legacy)
    
*   [Adding scores to traces](#adding-scores-to-traces)
    
*   [Interoperability with Langfuse Python SDK](#interoperability-with-langfuse-python-sdk)
    
*   [How it works](#how-it-works)
    
*   [Example](#example)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CCookbook%3A%20Langchain%20Integration%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/langchain/example-python.md)
Scroll to top

Docs

Integrations

Langchain

Example Python

This is a Jupyter notebook

[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/integration_langchain.ipynb)
[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/integration_langchain.ipynb)

Cookbook: Langchain Integration
===============================

This is a cookbook with examples of the Langfuse Integration for Langchain (Python).

Follow the [integration guide (opens in a new tab)](https://langfuse.com/docs/integrations/langchain)
 to add this integration to your Langchain project. The integration also supports Langchain JS.

Setup[](#setup)

----------------

    %pip install langfuse langchain langchain_openai --upgrade

Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment.

    import os
     
    # get keys for your project from https://cloud.langfuse.com
    os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-***"
    os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-***"
    os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # for EU data region
    # os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # for US data region
     
    # your openai key
    os.environ["OPENAI_API_KEY"] = "***"

    from langfuse.callback import CallbackHandler
     
    langfuse_handler = CallbackHandler()

    # Tests the SDK connection with the server
    langfuse_handler.auth_check()

Examples[](#examples)

----------------------

### Sequential Chain in Langchain Expression Language (LCEL)[](#sequential-chain-in-langchain-expression-language-lcel)

![Trace of Langchain LCEL](https://langfuse.com/images/docs/langchain_LCEL.png)

    from operator import itemgetter
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.schema import StrOutputParser
     
    langfuse_handler = CallbackHandler()
     
    prompt1 = ChatPromptTemplate.from_template("what is the city {person} is from?")
    prompt2 = ChatPromptTemplate.from_template(
        "what country is the city {city} in? respond in {language}"
    )
    model = ChatOpenAI()
    chain1 = prompt1 | model | StrOutputParser()
    chain2 = (
        {"city": chain1, "language": itemgetter("language")}
        | prompt2
        | model
        | StrOutputParser()
    )
     
    chain2.invoke({"person": "obama", "language": "spanish"}, config={"callbacks":[langfuse_handler]})

#### Runnable methods[](#runnable-methods)

Runnables are units of work that can be invoked, batched, streamed, transformed and composed.

The examples below show how to use the following methods with Langfuse:

*   invoke/ainvoke: Transforms a single input into an output.
    
*   batch/abatch: Efficiently transforms multiple inputs into outputs.
    
*   stream/astream: Streams output from a single input as it’s produced.
    

    # Async Invoke
    await chain2.ainvoke({"person": "biden", "language": "german"}, config={"callbacks":[langfuse_handler]})
     
    # Batch
    chain2.batch([{"person": "elon musk", "language": "english"}, {"person": "mark zuckerberg", "language": "english"}], config={"callbacks":[langfuse_handler]})
     
    # Async Batch
    await chain2.abatch([{"person": "jeff bezos", "language": "english"}, {"person": "tim cook", "language": "english"}], config={"callbacks":[langfuse_handler]})
     
    # Stream
    for chunk in chain2.stream({"person": "steve jobs", "language": "english"}, config={"callbacks":[langfuse_handler]}):
        print("Streaming chunk:", chunk)
     
    # Async Stream
    async for chunk in chain2.astream({"person": "bill gates", "language": "english"}, config={"callbacks":[langfuse_handler]}):
        print("Async Streaming chunk:", chunk)
     

### ConversationChain[](#conversationchain)

We'll use a [session (opens in a new tab)](https://langfuse.com/docs/tracing-features/sessions)
 in Langfuse to track this conversation with each invocation being a single trace.

In addition to the traces of each run, you also get a conversation view of the entire session:

![Session view of ConversationChain in Langfuse](https://langfuse.com/images/docs/langchain_session.png)

    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    from langchain_openai import OpenAI
     
    llm = OpenAI(temperature=0)
     
    conversation = ConversationChain(
        llm=llm, memory=ConversationBufferMemory()
    )

    # Create a callback handler with a session
    langfuse_handler = CallbackHandler(session_id="conversation_chain")

    conversation.predict(input="Hi there!", callbacks=[langfuse_handler])

    conversation.predict(input="How to build great developer tools?", callbacks=[langfuse_handler])

    conversation.predict(input="Summarize your last response", callbacks=[langfuse_handler])

### RetrievalQA[](#retrievalqa)

![Trace of Langchain QA Retrieval in Langfuse](https://langfuse.com/images/docs/langchain_qa_retrieval.jpg)

    import os
    os.environ["SERPAPI_API_KEY"] = ""

    %pip install unstructured selenium langchain-chroma --upgrade

    from langchain_community.document_loaders import SeleniumURLLoader
    from langchain_chroma import Chroma
    from langchain_text_splitters import CharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain.chains import RetrievalQA
     
    langfuse_handler = CallbackHandler()
     
    urls = [\
        "https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt",\
    ]
    loader = SeleniumURLLoader(urls=urls)
    llm = OpenAI()
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    docsearch = Chroma.from_documents(texts, embeddings)
    query = "What did the president say about Ketanji Brown Jackson"
    chain = RetrievalQA.from_chain_type(
        llm,
        retriever=docsearch.as_retriever(search_kwargs={"k": 1}),
    )
     
    chain.invoke(query, config={"callbacks":[langfuse_handler]})

### Agent[](#agent)

    from langchain.agents import AgentExecutor, load_tools, create_openai_functions_agent
    from langchain_openai import ChatOpenAI
    from langchain import hub
     
    langfuse_handler = CallbackHandler()
     
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    tools = load_tools(["serpapi"])
    prompt = hub.pull("hwchase17/openai-functions-agent")
    agent = create_openai_functions_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools)
     
    agent_executor.invoke({"input": "What is Langfuse?"}, config={"callbacks":[langfuse_handler]})

### AzureOpenAI[](#azureopenai)

    os.environ["AZURE_OPENAI_ENDPOINT"] = "<Azure OpenAI endpoint>"
    os.environ["AZURE_OPENAI_API_KEY"] = "<Azure OpenAI API key>"
    os.environ["OPENAI_API_TYPE"] = "azure"
    os.environ["OPENAI_API_VERSION"] = "2023-09-01-preview"

    from langchain_openai import AzureChatOpenAI
    from langchain.prompts import ChatPromptTemplate
     
    langfuse_handler = CallbackHandler()
     
    prompt = ChatPromptTemplate.from_template("what is the city {person} is from?")
    model = AzureChatOpenAI(
        deployment_name="gpt-35-turbo",
        model_name="gpt-3.5-turbo",
    )
    chain = prompt | model
     
    chain.invoke({"person": "Satya Nadella"}, config={"callbacks":[langfuse_handler]})

### Sequential Chain \[Legacy\][](#sequential-chain-legacy)

![Trace of Langchain Sequential Chain in Langfuse](https://langfuse.com/images/docs/langchain_chain.jpg)

    # further imports
    from langchain_openai import OpenAI
    from langchain.chains import LLMChain, SimpleSequentialChain
    from langchain.prompts import PromptTemplate
     
    llm = OpenAI()
    template = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
        Title: {title}
        Playwright: This is a synopsis for the above play:"""
    prompt_template = PromptTemplate(input_variables=["title"], template=template)
    synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)
    template = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
        Play Synopsis:
        {synopsis}
        Review from a New York Times play critic of the above play:"""
    prompt_template = PromptTemplate(input_variables=["synopsis"], template=template)
    review_chain = LLMChain(llm=llm, prompt=prompt_template)
    overall_chain = SimpleSequentialChain(
        chains=[synopsis_chain, review_chain],
    )
     
    # invoke
    review = overall_chain.invoke("Tragedy at sunset on the beach", {"callbacks":[langfuse_handler]}) # add the handler to the run method
    # run [LEGACY]
    review = overall_chain.run("Tragedy at sunset on the beach", callbacks=[langfuse_handler])# add the handler to the run method

Adding scores to traces[](#adding-scores-to-traces)

----------------------------------------------------

In addition to the attributes automatically captured by the decorator, you can add others to use the full features of Langfuse.

Two utility methods:

*   `langfuse_context.update_current_observation`: Update the trace/span of the current function scope
*   `langfuse_context.update_current_trace`: Update the trace itself, can also be called within any deeply nested span within the trace

For details on available attributes, have a look at the [reference (opens in a new tab)](https://python.reference.langfuse.com/langfuse/decorators#LangfuseDecorator.update_current_observation)
.

Below is an example demonstrating how to enrich traces and observations with custom parameters:

    from langfuse.decorators import langfuse_context, observe
     
    @observe(as_type="generation")
    def deeply_nested_llm_call():
        # Enrich the current observation with a custom name, input, and output
        langfuse_context.update_current_observation(
            name="Deeply nested LLM call", input="Ping?", output="Pong!"
        )
        # Set the parent trace's name from within a nested observation
        langfuse_context.update_current_trace(
            name="Trace name set from deeply_nested_llm_call",
            session_id="1234",
            user_id="5678",
            tags=["tag1", "tag2"],
            public=True
        )
     
    @observe()
    def nested_span():
        # Update the current span with a custom name and level
        langfuse_context.update_current_observation(name="Nested Span", level="WARNING")
        deeply_nested_llm_call()
     
    @observe()
    def main():
        nested_span()
     
    # Execute the main function to generate the enriched trace
    main()

On the Langfuse platform the trace now shows with the updated name from the `deeply_nested_llm_call`, and the observations will be enriched with the appropriate data points.

**Example trace:** [https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f16e0151-cca8-4d90-bccf-1d9ea0958afb (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f16e0151-cca8-4d90-bccf-1d9ea0958afb)

Interoperability with Langfuse Python SDK[](#interoperability-with-langfuse-python-sdk)

----------------------------------------------------------------------------------------

You can use this integration in combination with the `observe()` decorator from the Langfuse Python SDK. Thereby, you can trace non-Langchain code, combine multiple Langchain invocations in a single trace, and use the full functionality of the Langfuse Python SDK.

The `langfuse_context.get_current_langchain_handler()` method exposes a LangChain callback handler in the context of a trace or span when using `decorators`. Learn more about Langfuse Tracing [here (opens in a new tab)](https://langfuse.com/docs/tracing)
 and this functionality [here (opens in a new tab)](https://langfuse.com/docs/sdk/python/decorators#langchain)
.

### How it works[](#how-it-works)

    from langfuse.decorators import langfuse_context, observe
     
    # Create a trace via Langfuse decorators and get a Langchain Callback handler for it
    @observe() # automtically log function as a trace to Langfuse
    def main():
        # update trace attributes (e.g, name, session_id, user_id)
        langfuse_context.update_current_trace(
            name="custom-trace",
            session_id="user-1234",
            user_id="session-1234",
        )
        # get the langchain handler for the current trace
        langfuse_context.get_current_langchain_handler()
     
        # use the handler to trace langchain runs ...
     
    main()

### Example[](#example)

We'll run the same chain multiple times at different places within the hierarchy of a trace.

    TRACE: person-locator
    |
    |-- SPAN: Chain (Alan Turing)
    |
    |-- SPAN: Physics
    |   |
    |   |-- SPAN: Chain (Albert Einstein)
    |   |
    |   |-- SPAN: Chain (Isaac Newton)
    |   |
    |   |-- SPAN: Favorites
    |   |   |
    |   |   |-- SPAN: Chain (Richard Feynman)

Setup chain

    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
     
    prompt = ChatPromptTemplate.from_template("what is the city {person} is from?")
    model = ChatOpenAI()
     
    chain = prompt | model

Invoke it multiple times as part of a nested trace.

    from langfuse.decorators import langfuse_context, observe
     
    # On span "Physics"."Favorites"
    @observe()  # decorator to automatically log function as sub-span to Langfuse
    def favorites():
        # get the langchain handler for the current sub-span
        langfuse_handler = langfuse_context.get_current_langchain_handler()
        # invoke chain with langfuse handler
        chain.invoke({"person": "Richard Feynman"},
                     config={"callbacks": [langfuse_handler]})
     
    # On span "Physics"
    @observe()  # decorator to automatically log function as span to Langfuse
    def physics():
        # get the langchain handler for the current span
        langfuse_handler = langfuse_context.get_current_langchain_handler()
        # invoke chains with langfuse handler
        chain.invoke({"person": "Albert Einstein"},
                     config={"callbacks": [langfuse_handler]})
        chain.invoke({"person": "Isaac Newton"},
                     config={"callbacks": [langfuse_handler]})
        favorites()
     
    # On trace
    @observe()  # decorator to automatically log function as trace to Langfuse
    def main():
        # get the langchain handler for the current trace
        langfuse_handler = langfuse_context.get_current_langchain_handler()
        # invoke chain with langfuse handler
        chain.invoke({"person": "Alan Turing"},
                     config={"callbacks": [langfuse_handler]})
        physics()
     
    main()

View it in Langfuse

![Trace of Nested Langchain Runs in Langfuse](https://langfuse.com/images/docs/langchain_python_trace_interoperability.png)

[Tracing](/docs/integrations/langchain/tracing "Tracing")
[Example JS](/docs/integrations/langchain/example-javascript "Example JS")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#60131510100f1214200c010e07061513054e030f0d)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.