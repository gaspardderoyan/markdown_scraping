---
domain: langfuse.com
path: /docs/sdk/typescript/guide
url: https://langfuse.com/docs/sdk/typescript/guide
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Installation](#installation)
    
*   [End-to-end example](#end-to-end-example)
    
*   [Making calls](#making-calls)
    
*   [Create a trace](#trace)
    
*   [Observations](#observation)
    
*   [Create an Event](#event)
    
*   [Create a Span](#span)
    
*   [Create a Generation](#generation)
    
*   [Nesting of observations](#nesting)
    
*   [Create score](#score)
    
*   [Shutdown](#shutdown)
    
*   [Debugging](#debugging)
    
*   [Short lived execution environments (lambda, serverless, Vercel, Cloudflare)](#lambda)
    
*   [Option 1: Waiting for flushAsync but returning immediately](#option-1-waiting-for-flushasync-but-returning-immediately)
    
*   [Option 2: shutdownAsync](#option-2-shutdownasync)
    
*   [Upgrading from v2.x.x to v3.x.x](#upgrade2to3)
    
*   [Upgrading from v1.x.x to v2.x.x](#upgrade1to2)
    
*   [Dropped support for Node.js < 16](#dropped-support-for-nodejs--16)
    
*   [Rename prompt and completion to input and output](#rename-prompt-and-completion-to-input-and-output)
    
*   [More generalized usage object](#more-generalized-usage-object)
    
*   [Upgrading from v0.x to v1.x](#upgrade0to1)
    
*   [Deprecation of externalTraceId](#deprecation-of-externaltraceid)
    
*   [Introduction of shutdownAsync](#introduction-of-shutdownasync)
    
*   [Example](#example)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CJS%2FTS%20SDK%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/sdk/typescript/guide.mdx)
Scroll to top

Docs

SDKs

JS/TS

Guide

JS/TS SDK
=========

[![Github repository langfuse/langfuse-js](https://img.shields.io/badge/repo-langfuse--js-blue?style=flat-square&logo=Github)](https://github.com/langfuse/langfuse-js)
[![CI test status](https://img.shields.io/github/actions/workflow/status/langfuse/langfuse-js/ci.yml?style=flat-square&logo=Github&label=tests)](https://github.com/langfuse/langfuse-js/actions/workflows/ci.yml?query=branch%3Amain)
[![npm langfuse](https://img.shields.io/npm/v/langfuse?style=flat-square&label=npm+langfuse)](https://www.npmjs.com/package/langfuse)
[![npm langfuse-node](https://img.shields.io/npm/v/langfuse-node?style=flat-square&label=npm+langfuse-node)](https://www.npmjs.com/package/langfuse-node)

If you are working with Node.js, Deno, or Edge functions, the `langfuse` library is the simplest way to integrate Langfuse into your application. The library queues calls to make them non-blocking.

Supported runtimes:

*   [x]  Node.js
*   [x]  Edge: Vercel, Cloudflare, ...
*   [x]  Deno

Want to capture data (e.g. user feedback) from the browser? Use [LangfuseWeb](/docs/sdk/typescript/guide-web)

Installation[](#installation)

------------------------------

    npm i langfuse
    # or
    yarn add langfuse
     
    # Node.js < 18
    npm i langfuse-node
     
    # Deno
    import { Langfuse } from "https://esm.sh/langfuse"

Environment variablesConstructor parameters

.env

    LANGFUSE_SECRET_KEY="sk-lf-...";
    LANGFUSE_PUBLIC_KEY="pk-lf-...";
    LANGFUSE_BASEURL="https://cloud.langfuse.com"; # 🇪🇺 EU region
    # LANGFUSE_BASEURL="https://us.cloud.langfuse.com"; # 🇺🇸 US region

    import { Langfuse } from "langfuse"; // or "langfuse-node"
     
    // without additional options
    const langfuse = new Langfuse();
     
    // with additional options
    const langfuse = new Langfuse({
      release: "v1.0.0",
      requestTimeout: 10000,
    });

Optional constructor parameters:

| Variable | Description | Default value |
| --- | --- | --- |
| release | The release number/hash of the application to provide analytics grouped by release. | `process.env.LANGFUSE_RELEASE` or [common system environment names (opens in a new tab)](https://github.com/langfuse/langfuse-js/blob/main/langfuse-core/src/release-env.ts) |
| requestTimeout | Timeout in ms for requests | `10000` |
| enabled | Set to `false` to disable sending events | `true` if api keys are set, otherwise `false` |

In short-lived environments (e.g. serverless functions), make sure to always call `langfuse.shutdownAsync()` at the end to await all pending requests. ([Learn more](/docs/sdk/typescript/guide#lambda)
)

End-to-end example[](#end-to-end-example)

------------------------------------------

[JS SDK & Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)

Making calls[](#making-calls)

------------------------------

*   Each backend execution is logged with a single [trace](/docs/sdk/typescript/guide#trace)
    .
*   Each trace can contain multiple `observations` to log the individual steps of the execution.
    *   [Observations](/docs/sdk/typescript/guide#observations)
         can be of different types
        *   [Events](/docs/sdk/typescript/guide#event)
             are the basic building block. They are used to track discrete events in a trace.
        *   [Spans](/docs/sdk/typescript/guide#span)
             represent durations of units of work in a trace.
        *   [Generations](/docs/sdk/typescript/guide#generation)
             are spans which are used to log generations of AI model. They contain additional attributes about the model and the prompt/completion and are specifically rendered in the Langfuse UI.
    *   Observations can be [nested](/docs/sdk/typescript/guide#nesting)
        .

### Create a trace[](#trace)

Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.

    // Example trace creation
    const trace = langfuse.trace({
      name: "chat-app-session",
      userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
      metadata: { user: "[email protected]" },
      tags: ["production"],
    });
     
    // Example update, same params as create, cannot change id
    trace.update({
      metadata: {
        tag: "long-running",
      },
    });
     
    // Properties
    trace.id; // string
     
    // Create observations
    trace.event({});
    trace.span({});
    trace.generation({});
     
    // Add scores
    trace.score({});

**`langfuse.trace()` takes the following parameters**

| parameter | type | optional | description |
| --- | --- | --- | --- |
| id  | string | yes | The id of the trace can be set, defaults to a random id. Set it to link traces to external systems or when grouping multiple runs into a single trace (e.g. messages in a chat thread). |
| name | string | yes | Identifier of the trace. Useful for sorting/filtering in the UI. |
| input | object | yes | The input of the trace. Can be any JSON object. |
| output | object | yes | The output of the trace. Can be any JSON object. |
| metadata | object | yes | Additional metadata of the trace. Can be any JSON object. Metadata is merged when being updated via the API.object. |
| sessionId | string | yes | Used to group multiple traces into a [session](/docs/tracing-features/sessions)<br> in Langfuse. Use your own session/thread identifier. |
| userId | string | yes | The id of the user that triggered the execution. Used to provide [user-level analytics](/docs/tracing-features/users)<br>. |
| version | string | yes | The version of the trace type. Used to understand how changes to the trace type affect metrics. Useful in debugging. |
| tags | string\[\] | yes | Tags are used to categorize or label traces. Traces can be filtered by tags in the UI and GET API. Tags can also be changed in the UI. Tags are merged and never deleted via the API. |
| public | boolean | yes | You can make a trace `public` to share it via a [public link (opens in a new tab)](https://langfuse.com/docs/tracing-features/)<br>. This allows others to view the trace without needing to log in or be members of your Langfuse project. |

### Observations[](#observation)

*   `Events` are the basic building block. They are used to track discrete events in a trace.
*   `Spans` represent durations of units of work in a trace.
*   `Generations` are spans which are used to log generations of AI models. They contain additional attributes about the model, the prompt/completion. For generations, [token usage and model cost](/docs/model-usage-and-cost)
     are automatically calculated.
*   Observations can be nested.

#### Create an Event[](#event)

Events are used to track discrete events in a trace.

    // Example event
    const event = trace.event({
      name: "get-user-profile",
      metadata: {
        attempt: 2,
        httpRoute: "/api/retrieve-person",
      },
      input: {
        userId: "user__935d7d1d-8625-4ef4-8651-544613e7bd22",
      },
      output: {
        firstName: "Maxine",
        lastName: "Simons",
        email: "[email protected]",
      },
    });
     
    // Properties
    event.id; // string
    event.traceId; // string
    event.parentObservationId; // string | undefined
     
    // Create children
    event.event({});
    event.span({});
    event.generation({});
     
    // Add scores
    event.score({});

**`*.event()` takes the following parameters**

| parameter | type | optional | description |
| --- | --- | --- | --- |
| id  | string | yes | The id of the event can be set, defaults to a random id. |
| startTime | Date | yes | The time at which the event started, defaults to the current time. |
| name | string | yes | Identifier of the event. Useful for sorting/filtering in the UI. |
| metadata | object | yes | Additional metadata of the event. Can be any JSON object. Metadata is merged when being updated via the API. |
| level | string | yes | The level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI. |
| statusMessage | string | yes | The status message of the event. Additional field for context of the event. E.g. the error message of an error event. |
| input | object | yes | The input to the event. Can be any JSON object. |
| output | object | yes | The output to the event. Can be any JSON object. |
| version | string | yes | The version of the event type. Used to understand how changes to the event type affect metrics. Useful in debugging. |

#### Create a Span[](#span)

Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans as well as LLM spans.

    // Example span creation
    const span = trace.span({
      name: "embedding-retrieval",
      input: {
        userInput: "How does Langfuse work?",
      },
    });
     
    // Example update
    span.update({
      metadata: {
        httpRoute: "/api/retrieve-doc",
        embeddingModel: "bert-base-uncased",
      },
    });
     
    // Application code
    const retrievedDocs = await retrieveDoc("How does Langfuse work?");
     
    // Example end - sets endTime, optionally pass a body
    span.end({
      output: {
        retrievedDocs,
      },
    });
     
    // Properties
    span.id; // string
    span.traceId; // string
    span.parentObservationId; // string | undefined
     
    // Create children
    span.event({});
    span.span({});
    span.generation({});
     
    // Add scores
    span.score({});

**`*.span()` takes the following parameters**

| parameter | type | optional | description |
| --- | --- | --- | --- |
| id  | string | yes | The id of the span can be set, otherwise a random id is generated. |
| startTime | Date | yes | The time at which the span started, defaults to the current time. |
| endTime | Date | yes | The time at which the span ended. |
| name | string | yes | Identifier of the span. Useful for sorting/filtering in the UI. |
| metadata | object | yes | Additional metadata of the span. Can be any JSON object. Metadata is merged when being updated via the API. |
| level | string | yes | The level of the span. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI. |
| statusMessage | string | yes | The status message of the span. Additional field for context of the event. E.g. the error message of an error event. |
| input | object | yes | The input to the span. Can be any JSON object. |
| output | object | yes | The output to the span. Can be any JSON object. |
| version | string | yes | The version of the span type. Used to understand how changes to the span type affect metrics. Useful in debugging. |

#### Create a Generation[](#generation)

Generations are used to log generations of AI model. They contain additional attributes about the model and the prompt/completion and are specifically rendered in the Langfuse UI.

    // Example generation creation
    const generation = trace.generation({
      name: "chat-completion",
      model: "gpt-3.5-turbo",
      modelParameters: {
        temperature: 0.9,
        maxTokens: 2000,
      },
      input: messages,
    });
     
    // Application code
    const chatCompletion = await llm.respond(prompt);
     
    // Example update
    generation.update({
      completionStartTime: new Date(),
    });
     
    // Example end - sets endTime, optionally pass a body
    generation.end({
      output: chatCompletion,
    });
     
    // Properties
    generation.id; // string
    generation.traceId; // string
    generation.parentObservationId; // string | undefined
     
    // Create children
    generation.event({});
    generation.span({});
    generation.generation({});
     
    // Add scores
    generation.score({});

**`*.generation()` takes the following parameters**

| parameter | type | optional | description |
| --- | --- | --- | --- |
| id  | string | yes | The id of the generation can be set, defaults to random id. |
| name | string | yes | Identifier of the generation. Useful for sorting/filtering in the UI. |
| startTime | Date | yes | The time at which the generation started, defaults to the current time. |
| completionStartTime | Date | yes | The time at which the completion started (streaming). Set it to get latency analytics broken down into time until completion started and completion duration. |
| endTime | Date | yes | The time at which the generation ended. |
| model | string | yes | The name of the model used for the generation. |
| modelParameters | object | yes | The parameters of the model used for the generation; can be any key-value pairs. |
| input | object | yes | The input to the generation - the prompt. Can be any JSON object or string. |
| output | object | yes | The output to the generation - the completion. Can be any JSON object or string. |
| usage | object | yes | The usage object supports the OpenAi structure with (`promptTokens`, `completionTokens`, `totalTokens`) and a more generic version (`input`, `output`, `total`, `unit`, `inputCost`, `outputCost`, `totalCost`) where unit can be of value `"TOKENS"`, `"CHARACTERS"`, `"MILLISECONDS"`, `"SECONDS"`, `"IMAGES"`. Refer to the docs on how to [automatically calculate (opens in a new tab)](https://langfuse.com/docs/model-usage-and-cost)<br> tokens and costs by Langfuse. |
| metadata | object | yes | Additional metadata of the generation. Can be any JSON object. Metadata is merged when being updated via the API. |
| level | string | yes | The level of the generation. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI. |
| statusMessage | string | yes | The status message of the generation. Additional field for context of the event. E.g. the error message of an error event. |
| version | string | yes | The version of the generation type. Used to understand how changes to the generation type affect metrics. Reflects e.g. the version of a prompt. |
| prompt | Langfuse prompt | yes | Pass the prompt fetched from [Langfuse Prompt Management](/docs/prompts)<br> via `langfuse.getPrompt()` in order to link the generation to a specific prompt version for analytics in Langfuse. |

#### Nesting of observations[](#nesting)

Nesting of observations (spans, events, generations) is helpful to structure the trace in a hierarchical way.

    # Simple example; there are no limits to how you nest observations
    - trace: chat-app-session
      - span: chat-interaction
        - event: get-user-profile
        - generation: chat-completion

There are two options to nest observations:

DefaultManual nesting via ids (distributed traces)

    const trace = langfuse.trace({ name: "chat-app-session" });
     
    const span = trace.span({ name: "chat-interaction" });
     
    span.event({ name: "get-user-profile" });
    span.generation({ name: "chat-completion" });

### Create score[](#score)

Scores are used to evaluate executions/traces. They are attached to a single [trace](/docs/sdk/typescript/guide#trace)
. If the score relates to a specific step of the trace, the score can optionally also be attached to the observation to enable evaluating it specifically.

Links

*   Learn more about [Scores in Langfuse](/docs/scores)
    
*   Report scores from the browser (e.g. user feedback) using the [Web SDK](/docs/sdk/typescript/guide-web)
    

    await langfuse.score({
      traceId: message.traceId,
      observationId: message.generationId,
      name: "quality",
      value: 1,
      comment: "Factually correct",
    });
     
    // alternatively
    trace.score({});
    span.score({});
    event.score({});
    generation.score({});

| parameter | type | optional | description |
| --- | --- | --- | --- |
| traceId | string | no  | The id of the trace to which the score should be attached. Automatically set if you use `{trace,generation,span,event}.score({})` |
| observationId | string | yes | The id of the observation to which the score should be attached. Automatically set if you use `{generation,span,event}.score({})` |
| name | string | no  | Identifier of the score. |
| value | number | no  | The value of the score. Can be any number, often standardized to 0..1 |
| comment | string | yes | Additional context/explanation of the score. |

Shutdown[](#shutdown)

----------------------

The Langfuse SDKs sends events asynchronously to the Langfuse server. You should call shutdown to exit cleanly before your application exits.

    langfuse.shutdown();
    // or
    await langfuse.shutdownAsync();

Debugging[](#debugging)

------------------------

Issues with the SDKs can be caused by various reasons ranging from incorrectly configured API keys to network issues.

The SDK does not throw errors to protect your application process. Instead, you can optionally listen to errors:

    langfuse.on("error", (error) => {
      // Whatever you want to do with the error
      console.error(error);
    });

Alternatively, you can enable debugging to get detailed logs of what's happening in the SDK.

    langfuse.debug();

Short lived execution environments (lambda, serverless, Vercel, Cloudflare)[](#lambda)

---------------------------------------------------------------------------------------

The SDK is optimize to run in the background to queue and flush all requests. Events can get lost if the process exits before all requests are flushed. To ensure all events are sent, use one of the following patterns:

### Option 1: Waiting for `flushAsync` but returning immediately[](#option-1-waiting-for-flushasync-but-returning-immediately)

Some platforms/frameworks support waiting for promises after the response but before exiting the process. This is the preferred way to ensure all events are sent without blocking the process.

Note: Most of these execution environments have a timeout after which the process is killed. Some of them lack observability to monitor these dangling promises.

*   Cloudflare workers: [`waitUntil` (opens in a new tab)](https://developers.cloudflare.com/workers/runtime-apis/handlers/fetch)
     ([mdn (opens in a new tab)](https://developer.mozilla.org/en-US/docs/Web/API/ExtendableEvent/waitUntil)
    , [example (opens in a new tab)](https://gist.github.com/arunesh90/3b0fef2c1c77b106b14ebba9ea26321c)
    )
    
*   Vercel (e.g. NextJs): [`waitUntil` (opens in a new tab)](https://vercel.com/docs/functions/functions-api-reference#waituntil)
    
        npm i @vercel/functions
    
        import { waitUntil } from "@vercel/functions";
        // within the api handler
        waitUntil(langfuse.flushAsync());
    

### Option 2: `shutdownAsync`[](#option-2-shutdownasync)

When the process exits use `await langfuse.shutdownAsync()` to make sure all requests are flushed and pending requests are awaited. Call this once at the end of your process.

Example:

    const langfuse = new Langfuse({
      secretKey: "sk-lf-...",
      publicKey: "pk-lf-...",
    })
     
    export const handler() {
      const trace = langfuse.trace({ name: "chat-app-session" });
     
      trace.event({ name: "get-user-profile" });
      const span = trace.span({ name: "chat-interaction" });
      span.generation({ name: "chat-completion", model: "gpt-3.5-turbo", input: prompt, output: completion });
     
      // So far all requests are queued
     
      // Now we want to flush and await all pending requests before the process exits
      await langfuse.shutdownAsync();
    }

Upgrading from v2.x.x to v3.x.x[](#upgrade2to3)

------------------------------------------------

This release includes breaking changes only for users of the [Langchain JS integration](/docs/integrations/langchain/typescript)
. The upgrade is non-breaking for all other parts of the SDK.

Upgrading from v1.x.x to v2.x.x[](#upgrade1to2)

------------------------------------------------

You can automatically migrate your codebase using [grit (opens in a new tab)](https://www.grit.io/)
, either [online (opens in a new tab)](https://app.grit.io/migrations/new/langfuse_node_v2)
 or with the following CLI command:

    
    npx @getgrit/launcher apply langfuse_node_v2
    

The grit binary executes entirely locally with AST-based transforms. Be sure to audit its changes: we suggest ensuring you have a clean working tree beforehand, and running `git add --patch` afterwards.

### Dropped support for Node.js < 16[](#dropped-support-for-nodejs--16)

As most of our users are using a modern JS/TS stack, we decided to drop support for Node < 16.

### Rename `prompt` and `completion` to `input` and `output`[](#rename-prompt-and-completion-to-input-and-output)

To ensure consistency throughout Langfuse, we have renamed the `prompt` and `completion` parameters in the `generation` function to `input` and `output`, respectively. This change brings them in line with the rest of the Langfuse API.

### More generalized usage object[](#more-generalized-usage-object)

We improved the flexibility of the SDK by allowing you to ingest any type of usage while still supporting the OpenAI-style usage object.

**v1.x.x**

    langfuse.generation({
      name: "chat-completion",
      usage = {
        promptTokens: 50,
        completionTokens: 49,
        totalTokens: 99,
      },
    });

**v2.x.x**

The usage object supports the OpenAi structure with `{'promptTokens', 'completionTokens', 'totalTokens'}` and a more generic version `{'input', 'output', 'total', 'unit'}` where unit can be of value `"TOKENS"`, `"CHARACTERS"`, `"MILLISECONDS"`, `"SECONDS"`, `"IMAGES"`. For some models the token counts are [automatically calculated (opens in a new tab)](https://langfuse.com/docs/model-usage-and-cost)
 by Langfuse. Create an issue to request support for other units and models.

    // Generic style
    langfuse.generation({
      name = "my-claude-generation",
      usage = {
        input: 50,
        output: 49,
        total: 99,
        unit: "TOKENS",
      },
    });
     
    // OpenAI style
    langfuse.generation({
      name = "my-openai-generation",
      usage = {
        promptTokens: 50,
        completionTokens: 49,
        totalTokens: 99,
      }, // defaults to "TOKENS" unit
    });
     
    // set ((input and/or output) or total), total is calculated automatically if not set

Upgrading from v0.x to v1.x[](#upgrade0to1)

--------------------------------------------

### Deprecation of `externalTraceId`[](#deprecation-of-externaltraceid)

We deprecated the external trace id to simplify the API. Instead, you can now (optionally) directly set the trace id when creating the trace. Traces are still upserted in case a trace with this id already exists in your project.

    // v0.x
    const trace = langfuse.trace({ externalId: "123" });
    // When manually linking observations and scores to the trace
    const span = langfuse.span({ traceId: "123", traceIdType: "EXTERNAL" });
    const score = langfuse.score({ traceId: "123", traceIdType: "EXTERNAL" });
     
    // v1.x
    const trace = langfuse.trace({ id: "123" });
    // When manually linking observations and scores to the trace
    const span = langfuse.span({ traceId: "123" });
    const score = langfuse.score({ traceId: "123" });

Changes

*   The `traceIdType` property is deprecated
*   The `externalId` property on traces is deprecated

Ingestion of externalIds via older versions of the SDK or the API is still supported. However, we will remove support for this in the future and migrate all existing traces to the new format. We monitor the usage of deprecated properties on Langfuse Cloud and will reach out to you if we detect that you are still using them before a breaking change is introduced.

### Introduction of `shutdownAsync`[](#introduction-of-shutdownasync)

With v1.0.0 we introduced the `shutdownAsync` method to make sure all requests are flushed and pending requests are awaited before the process exits. `flush` is still available but does not await pending requests that are already flushed.

This is especially important for short-lived execution environments such as [lambdas and serverless functions](/docs/sdk/typescript/guide#lambda)
.

    export const handler() {
      // Lambda / serverless function
     
      // v0.x
      await langfuse.flush();
     
      // v1.x
      await langfuse.shutdownAsync();
    }

Example[](#example)

--------------------

We integrated the Typescript SDK into the Vercel AI Chatbot project. Check out the [blog post](/blog/showcase-llm-chatbot)
 for screenshots and detailed explanations of the inner workings of the integration. The project includes:

*   Streamed responses from OpenAI
*   Conversations
*   Collection of user feedback on individual messages using the Web SDK

[Reference ↗](/docs/sdk/typescript/guide# "Reference ↗")
[Guide (Web)](/docs/sdk/typescript/guide-web "Guide (Web)")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#03707673736c7177436f626d64657670662d606c6e)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.