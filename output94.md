---
domain: langfuse.com
path: /docs/prompts/example-openai-functions
url: https://langfuse.com/docs/prompts/example-openai-functions
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Setup](#setup)
    
*   [Add prompt to Langfuse Prompt Management](#add-prompt-to-langfuse-prompt-management)
    
*   [Example application](#example-application)
    
*   [Get current prompt version from Langfuse](#get-current-prompt-version-from-langfuse)
    
*   [Create example function](#create-example-function)
    
*   [Execute it](#execute-it)
    
*   [View trace in Langfuse](#view-trace-in-langfuse)
    
*   [Iterate on prompt in Langfuse](#iterate-on-prompt-in-langfuse)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CExample%3A%20Langfuse%20Prompt%20Management%20for%20OpenAI%20functions%20(Python)%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/prompts/example-openai-functions.md)
Scroll to top

Docs

Prompt Management

Example OpenAI Functions

This is a Jupyter notebook

[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/prompt_management_openai_functions.ipynb)
[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/prompt_management_openai_functions.ipynb)

Example: Langfuse Prompt Management for OpenAI functions (Python)
=================================================================

Langfuse [Prompt Management (opens in a new tab)](https://langfuse.com/docs/prompts)
 helps to version control and manage prompts collaboratively in one place. This example demostrates how to use the flexible `config` object on Langfuse prompts to store function calling options and model parameters.

Setup[](#setup)

----------------

    %pip install langfuse openai --upgrade

    import os
     
    # Get keys for your project
    os.environ["LANGFUSE_PUBLIC_KEY"] = ""
    os.environ["LANGFUSE_SECRET_KEY"] = ""
    os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"
     
    # OpenAI key
    os.environ["OPENAI_API_KEY"] = ""

    from langfuse import Langfuse
    langfuse = Langfuse()
     
    # optional, verify that Langfuse is configured correctly
    langfuse.auth_check()

    True

Add prompt to Langfuse Prompt Management[](#add-prompt-to-langfuse-prompt-management)

--------------------------------------------------------------------------------------

We add the prompt used in this example via the SDK. Alternatively, you can also edit and version the prompt in the Langfuse UI.

*   `Name` that identifies the prompt in Langfuse Prompt Management
*   Prompt with `json_schema` variable
*   Config including `model_name`, `temperature`, and `json_schema`
*   `labels` to include `production` to immediately use the prompt as the default

    langfuse.create_prompt(
        name="story_summarization",
        prompt="Extract the key information from this text and return it in JSON format. Use the following schema: {{json_schema}}",
        config={
            "model":"gpt-3.5-turbo-1106",
            "temperature": 0,
            "json_schema":{
                "main_character": "string (name of protagonist)",
                "key_content": "string (1 sentence)",
                "keywords": "array of strings",
                "genre": "string (genre of story)",
                "critic_review_comment": "string (write similar to a new york times critic)",
                "critic_score": "number (between 0 bad and 10 exceptional)"
            }
        },
        labels=["production"]
    );

Prompt in Langfuse UI

![Langfuse Prompt Management](https://langfuse.com/images/docs/prompt-management-with-config-for-openai-functions.png)

Example application[](#example-application)

--------------------------------------------

### Get current prompt version from Langfuse[](#get-current-prompt-version-from-langfuse)

    prompt = langfuse.get_prompt("story_summarization")

We can now use the prompt to compile our system message

    prompt.compile(json_schema="TEST SCHEMA")

    'Extract the key information from this text and return it in JSON format. Use the following schema: TEST SCHEMA'

And it includes the config object

    prompt.config

    {'model': 'gpt-3.5-turbo-1106',
     'json_schema': {'genre': 'string (genre of story)',
      'keywords': 'array of strings',
      'key_content': 'string (1 sentence)',
      'critic_score': 'number (between 0 bad and 10 exceptional)',
      'main_character': 'string (name of protagonist)',
      'critic_review_comment': 'string (write similar to a new york times critic)'},
     'temperature': 0}

### Create example function[](#create-example-function)

In this example we use the native [Langfuse OpenAI integration (opens in a new tab)](https://langfuse.com/docs/integrations/openai)
 by importing from `langfuse.openai`. This enables [tracing (opens in a new tab)](https://langfuse.com/docs/tracing)
 in Langfuse and is not required for using Langfuse prompts management.

    from langfuse.openai import OpenAI
    client = OpenAI()

Use Langfuse prompt to construct the `summarize_story` example function.

    import json
     
    def summarize_story(story):
      # Stringify the JSON schema
      json_schema_str = ', '.join([f"'{key}': {value}" for key, value in prompt.config["json_schema"].items()])
     
      # Compile prompt with stringified version of json schema
      system_message = prompt.compile(json_schema=json_schema_str)
     
      # Format as OpenAI messages
      messages = [\
          {"role":"system","content": system_message},\
          {"role":"user","content":story}\
      ]
     
      # Get additional config
      model = prompt.config["model"]
      temperature = prompt.config["temperature"]
     
      # Execute LLM call
      res = client.chat.completions.create(
        model = model,
        temperature = temperature,
        messages = messages,
        response_format = { "type": "json_object" },
        langfuse_prompt = prompt # capture used prompt version in trace
      )
     
      # Parse response as JSON
      res = json.loads(res.choices[0].message.content)
     
      return res

### Execute it[](#execute-it)

    # Thanks ChatGPT for the story
    STORY = """
    In a bustling city where the nighttime glittered with neon signs and the rush never calmed, lived a lonely cat named Whisper. Amidst the ceaseless clatter, Whisper discovered an abandoned hat one day. To her enigmatic surprise, this was no ordinary accessory; it had the unusual power to make her invisible to any onlooker.
    Whisper, now carrying a peculiar power, started a journey that was unexpected. She became a benevolent spirit to the less fortunate, the homeless people who equally shared the cold nights with her. Nights that were once barren turned miraculous as warm meals mysteriously appeared to those who needed them most. No one could see her, yet her actions spoke volumes, turning her into an unsung hero in the hidden corners of the city.
    As she carried on with her mysterious deed, she found an unanticipated reward. Joy started to kindle in her heart, born not from the invisibility, but from the result of her actions; the growing smiles on the faces of those she surreptitiously helped. Whisper might have remained unnoticed to the world, but amidst her secret kindness, she discovered her true happiness.
    """

    summary = summarize_story(STORY)

    {'genre': 'Fantasy',
     'keywords': ['lonely cat',\
      'invisible',\
      'benevolent spirit',\
      'unsung hero',\
      'mysterious deed',\
      'true happiness'],
     'key_content': 'In a bustling city, a lonely cat named Whisper discovers an abandoned hat with the power to make her invisible, leading her to become a benevolent spirit and unsung hero to the less fortunate.',
     'critic_score': 9,
     'main_character': 'Whisper',
     'critic_review_comment': "Whisper's journey from loneliness to self-discovery through acts of kindness is a heartwarming and enchanting tale that captivates the reader with its magical elements and profound message about true happiness."}

View trace in Langfuse[](#view-trace-in-langfuse)

--------------------------------------------------

As we used the native Langfuse integration with the OpenAI SDK, we can view the trace in Langfuse.

![Trace of OpenAI functions in Langfuse](https://langfuse.com/images/docs/openai-functions-trace-with-prompt-management.png)

Iterate on prompt in Langfuse[](#iterate-on-prompt-in-langfuse)

----------------------------------------------------------------

We can now iterate on the prompt in Langfuse UI including model parameters and function calling options without changing the code or redeploying the application.

[Get Started](/docs/prompts/get-started "Get Started")
[Example Langchain (Py)](/docs/prompts/example-langchain "Example Langchain (Py)")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#b6c5c3c6c6d9c4c2f6dad7d8d1d0c3c5d398d5d9db)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.