---
domain: langfuse.com
path: /docs/integrations/litellm
url: https://langfuse.com/docs/integrations/litellm
---

[Join us in Engineering & DevRel â†’We're hiring. Join us in Product Eng, Backend Eng, and DevRel â†’](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference â†— (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference â†— (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API â†— (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK â†— (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK â†— (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support â†— (opens in a new tab)](/support)
    

Light

On This Page

*   [1\. LiteLLM Proxy + Langfuse OpenAI SDK Wrapper](#1-litellm-proxy--langfuse-openai-sdk-wrapper)
    
*   [2\. Send Logs from LiteLLM Proxy to Langfuse](#2-send-logs-from-litellm-proxy-to-langfuse)
    
*   [3\. LiteLLM Python SDK](#3-litellm-python-sdk)
    
*   [Quick Example](#quick-example)
    
*   [Use within decorated function](#use-within-decorated-function)
    
*   [Set Custom Trace ID, Trace User ID and Tags](#set-custom-trace-id-trace-user-id-and-tags)
    
*   [Use LangChain ChatLiteLLM + Langfuse](#use-langchain-chatlitellm--langfuse)
    
*   [Customize Langfuse Python SDK via Environment Variables](#customize-langfuse-python-sdk-via-environment-variables)
    

[Question? Give us feedback â†’ (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9COSS%20Observability%20for%20LiteLLM%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/litellm/tracing.mdx)
Scroll to top

Docs

Integrations

LiteLLM

Tracing

ðŸš… LiteLLM Integration
======================

**LiteLLM** ([GitHub (opens in a new tab)](https://github.com/BerriAI/litellm)
): Use any LLM as a drop in replacement for GPT. Use Azure, OpenAI, Cohere, Anthropic, Ollama, VLLM, Sagemaker, HuggingFace, Replicate (100+ LLMs).

There are three ways to integrate LiteLLM with Langfuse:

1.  LiteLLM Proxy with OpenAI SDK Wrapper, the proxy standardizes 100+ models on the OpenAI API schema and the Langfuse OpenAI SDK wrapper instruments the LLM calls.
2.  LiteLLM Proxy which can send logs to Langfuse if enabled in the UI.
3.  LiteLLM Python SDK which can send logs to Langfuse if the environment variables are set.

Example trace in Langfuse using multiple models via LiteLLM:

* * *

1\. LiteLLM Proxy + Langfuse OpenAI SDK Wrapper[](#1-litellm-proxy--langfuse-openai-sdk-wrapper)

-------------------------------------------------------------------------------------------------

This is the preferred way to integrate LiteLLM with Langfuse. The Langfuse OpenAI SDK wrapper automatically captures token counts, latencies, streaming response times (time to first token), API errors, and more.

How this works:

1.  The [LiteLLM Proxy (opens in a new tab)](https://docs.litellm.ai/docs/simple_proxy)
     standardizes 100+ models on the OpenAI API schema
2.  and the Langfuse OpenAI SDK wrapper ([Python](/docs/integrations/openai/python)
    , [JS/TS](/docs/integrations/openai/js)
    ) instruments the LLM calls.

To see a full end-to-end example, check out the LiteLLM cookbook.

[Python Cookbook](/docs/integrations/litellm/example-proxy-python)
[JS/TS Cookbook](/docs/integrations/litellm/example-proxy-js)

2\. Send Logs from LiteLLM Proxy to Langfuse[](#2-send-logs-from-litellm-proxy-to-langfuse)

--------------------------------------------------------------------------------------------

By setting the callback to Langfuse in the LiteLLM UI you can instantly log your responses across all providers. For more information on how to setup the Proxy UI, see the [LiteLLM docs (opens in a new tab)](https://docs.litellm.ai/docs/proxy/ui)
.

![Set Langfuse as callback in Proxy UI](https://langfuse.com/images/docs/litellm-ui.png)

* * *

3\. LiteLLM Python SDK[](#3-litellm-python-sdk)

------------------------------------------------

Instead of the proxy, you can also use the native LiteLLM Python client. You can find more in-depth documentation in the [LiteLLM docs (opens in a new tab)](https://litellm.vercel.app/docs/observability/langfuse_integration)
.

    pip install langfuse>=2.0.0 litellm

main.py

    from litellm import completion
     
    ## set env variables
    os.environ["LANGFUSE_PUBLIC_KEY"] = ""
    os.environ["LANGFUSE_SECRET_KEY"] = ""
     
    # Langfuse host
    os.environ["LANGFUSE_HOST"]="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
    # os.environ["LANGFUSE_HOST"]="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region
     
    os.environ["OPENAI_API_KEY"] = ""
    os.environ["COHERE_API_KEY"] = ""
     
    # set callbacks
    litellm.success_callback = ["langfuse"]
    litellm.failure_callback = ["langfuse"]
     

### Quick Example[](#quick-example)

main.py

    import litellm
     
    # openai call
    openai_response = litellm.completion(
      model="gpt-3.5-turbo",
      messages=[\
        {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}\
      ]
    )
     
    print(openai_response)
     
    # cohere call
    cohere_response = litellm.completion(
      model="command-nightly",
      messages=[\
        {"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}\
      ]
    )
    print(cohere_response)
     

### Use within decorated function[](#use-within-decorated-function)

If you want to use the LiteLLM SDK within a decorated function ([observe() decorator](/docs/sdk/python/decorators)
), you can use the `langfuse_context.get_current_trace_id()` method to get the current trace ID and pass it to the LiteLLM SDK.

main.py

    from litellm import completion
    from langfuse.decorators import langfuse_context, observe
     
    @observe()
    def fn():
      # set custom langfuse trace params and generation params
      response = completion(
        model="gpt-3.5-turbo",
        messages=[\
          {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}\
        ],
        metadata={
            "trace_id": langfuse_context.get_current_trace_id(),   # set langfuse trace ID
        },
      )
     
      print(response)

[GitHub issue (opens in a new tab)](https://github.com/langfuse/langfuse/issues/2238)
 tracking a native integration that will automatically capture nested traces when the LiteLLM SDK is used within a decorated function.

### Set Custom Trace ID, Trace User ID and Tags[](#set-custom-trace-id-trace-user-id-and-tags)

main.py

    from litellm import completion
     
    # set custom langfuse trace params and generation params
    response = completion(
      model="gpt-3.5-turbo",
      messages=[\
        {"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}\
      ],
      metadata={
          "generation_name": "test-generation",   # set langfuse Generation Name
          "generation_id": "gen-id",              # set langfuse Generation ID
          "trace_id": "trace-id",                 # set langfuse Trace ID
          "trace_user_id": "user-id",             # set langfuse Trace User ID
          "session_id": "session-id",             # set langfuse Session ID
          "tags": ["tag1", "tag2"]                # set langfuse Tags
      },
    )
     
    print(response)
     

### Use LangChain ChatLiteLLM + Langfuse[](#use-langchain-chatlitellm--langfuse)

    pip install langchain

main.py

    from langchain.chat_models import ChatLiteLLM
    from langchain.schema import HumanMessage
    import litellm
     
     
    chat = ChatLiteLLM(
      model="gpt-3.5-turbo"
      model_kwargs={
          "metadata": {
            "trace_user_id": "user-id", # set Langfuse Trace User ID
            "session_id": "session-id", # set Langfuse Session ID
            "tags": ["tag1", "tag2"] # set Langfuse Tags
          }
        }
      )
    messages = [\
        HumanMessage(\
            content="what model are you"\
        )\
    ]
    chat(messages)
     

### Customize Langfuse Python SDK via Environment Variables[](#customize-langfuse-python-sdk-via-environment-variables)

To customise Langfuse settings, use the [Langfuse environment variables](/docs/sdk/python/low-level-sdk#initialize-client)
. These will be picked up by the LiteLLM SDK on initialization as it uses the Langfuse Python SDK under the hood.

[Example (Python)](/docs/integrations/haystack/example-python "Example (Python)")
[Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python "Example Proxy (Python)")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#bccfc9ccccd3cec8fcd0ddd2dbdac9cfd992dfd3d1)
[Talk to sales](/schedule-demo)

### Subscribe to updates

GetÂ updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

Â© 2022-2024 Langfuse GmbH / Finto Technologies Inc.