---
domain: langfuse.com
path: /docs/prompts/get-started
url: https://langfuse.com/docs/prompts/get-started
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Why use prompt management?](#why-use-prompt-management)
    
*   [Langfuse prompt object](#langfuse-prompt-object)
    
*   [How it works](#how-it-works)
    
*   [Create/update prompt](#createupdate-prompt)
    
*   [Use prompt](#use-prompt)
    
*   [Link with Langfuse Tracing (optional)](#link-with-langfuse-tracing-optional)
    
*   [Rollbacks (optional)](#rollbacks-optional)
    
*   [End-to-end examples](#end-to-end-examples)
    
*   [Performance](#performance)
    
*   [Caching in client SDKs](#caching-in-client-sdks)
    
*   [Performance measurement (excluding cache hits)](#performance-measurement-excluding-cache-hits)
    
*   [Optional: Fallback for guaranteed availability](#fallback)
    
*   [FAQ](#faq)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CPrompt%20Management%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/prompts/get-started.mdx)
Scroll to top

Docs

Prompt Management

Get Started

Prompt Management
=================

Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is basically a **Prompt CMS** (Content Management System).

Why use prompt management?[](#why-use-prompt-management)

---------------------------------------------------------

> Can't I just hardcode my prompts in my application and track them in Git? Yes, well... you can and all of us have done it.

Typical benefits of using a CMS apply here:

*   Decoupling: deploy new prompts without redeploying your application.
*   Non-technical users can create and update prompts via Langfuse Console.
*   Quickly rollback to a previous version of a prompt.

Platform benefits:

*   Track performance of prompt versions in Langfuse Tracing.

Langfuse prompt object[](#langfuse-prompt-object)

--------------------------------------------------

Example prompt in Langfuse with custom config

    {
      "name": "movie-critic",
      "type": "text",
      "prompt": "Do you like {{movie}}?",
      "config": {
        "model": "gpt-3.5-turbo",
        "temperature": 0.5,
        "supported_languages": ["en", "fr"]
      },
      "version": 1,
      "labels": ["production", "staging", "latest"],
      "tags": ["movies"]
    }

*   `name`: Unique name of the prompt within a Langfuse project.
*   `type`: The type of the prompt content (`text` or `chat`). Default is `text`.
*   `prompt`: The text template with variables (e.g. `This is a prompt with a {{variable}}`). For chat prompts, this is a list of chat messages each with `role` and `content`.
*   `config`: Optional JSON object to store any parameters (e.g. model parameters or model tools).
*   `version`: Integer to indicate the version of the prompt. The version is automatically incremented when creating a new prompt version.
*   `labels`: Labels that can be used to fetch specific prompt versions in the SDKs. The default-served prompt version is the one with the `production` label. The `latest` label is automatically maintained by Langfuse and points to the latest version. Labels are unique per prompt name.

How it works[](#how-it-works)

------------------------------

### Create/update prompt[](#createupdate-prompt)

If you already have a prompt with the same `name`, the prompt will be added as a new version.

Langfuse UIPythonJS/TS

### Use prompt[](#use-prompt)

At runtime, you can fetch the latest production version from Langfuse.

PythonJS/TSLangchain (Python)Langchain (JS)

    from langfuse import Langfuse
     
    # Initialize Langfuse client
    langfuse = Langfuse()
     
    # Get current production version of a text prompt
    prompt = langfuse.get_prompt("movie-critic")
     
    # Insert variables into prompt template
    compiled_prompt = prompt.compile(movie="Dune 2")
    # -> "Do you like Dune 2?"

Chat prompts

    # Get current production version of a chat prompt
    chat_prompt = langfuse.get_prompt("movie-critic-chat", type="chat") # type arg infers the prompt type (default is 'text')
     
    # Insert variables into chat prompt template
    compiled_chat_prompt = chat_prompt.compile(movie="Dune 2")
    # -> [{"role": "system", "content": "You are an expert on Dune 2"}]

Optional parameters

    # Get specific version
    prompt = langfuse.get_prompt("movie-critic", version=1)
     
    # Get specific label
    prompt = langfuse.get_prompt("movie-critic", label="staging")
     
    # Get latest prompt version. The 'latest' label is automatically maintained by Langfuse.
    prompt = langfuse.get_prompt("movie-critic", label="latest")
     
    # Extend cache TTL from default 60 to 300 seconds
    prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)
     
    # Number of retries on fetching prompts from the server. Default is 2.
    prompt = langfuse.get_prompt("movie-critic", max_retries=3)
     
    # Timeout per call to the Langfuse API in seconds. Default is 20.
    prompt = langfuse.get_prompt("movie-critic", fetch_timeout_seconds=3)

Attributes

    # Raw prompt including {{variables}}. For chat prompts, this is a list of chat messages.
    prompt.prompt
     
    # Config object
    prompt.config

### Link with Langfuse Tracing (optional)[](#link-with-langfuse-tracing-optional)

Add the prompt object to the `generation` call in the SDKs to link the generation in [Langfuse Tracing](/docs/tracing)
 to the prompt version. This linkage enables tracking of metrics by prompt version and name, such as "movie-critic", directly in the Langfuse UI. Metrics like scores per prompt version provide insights into how modifications to prompts impact the quality of the generations. If a [fallback prompt](/docs/prompts/get-started#fallback)
 is used, no link will be created.

This is currently unavailable when using the LangChain or LlamaIndex integration.

Python SDKJS/TS SDKOpenAI SDK (Python)OpenAI SDK (JS/TS)

Decorators

    from langfuse.decorators import langfuse_context, observe
     
    @observe(as_type="generation")
    def nested_generation():
        prompt = langfuse.get_prompt("movie-critic")
     
        langfuse_context.update_current_observation(
            prompt=prompt,
        )
     
    @observe()
    def main():
      nested_generation()
     
    main()

Low-level SDK

    langfuse.generation(
        ...
    +   prompt=prompt
        ...
    )

### Rollbacks (optional)[](#rollbacks-optional)

When a prompt has a `production` label, then that version will be served by default in the SDKs. You can quickly rollback to a previous version by setting the `production` label to that previous version in the Langfuse UI.

End-to-end examples[](#end-to-end-examples)

--------------------------------------------

The following example notebooks include end-to-end examples of prompt management:

[Example OpenAI Functions](/docs/prompts/example-openai-functions)
[Example Langchain (Python)](/docs/prompts/example-langchain)
[Example Langchain (JS/TS)](/docs/prompts/example-langchain-js)

We also used Prompt Management for our Docs Q&A Chatbot and traced it with Langfuse. You can get view-only access to the project by signing up to the [public demo](/docs/demo)
.

Performance[](#performance)

----------------------------

### Caching in client SDKs[](#caching-in-client-sdks)

While [Langfuse Tracing](/docs/tracing)
 is fully asynchronous and non-blocking, managing prompts in Langfuse adds latency to your application when retrieving the prompt. To minimize the impact on your application, prompts are cached in the client SDKs. The default cache TTL is 60 seconds and is configurable.

When refetching a prompt fails but an expired version is in the cache, the SDKs will return the expired version, preventing application blockage due to network issues. If you'd like to have a safety net on the first fetch where the cache is empty, you can provide a [fallback prompt](/docs/prompts/get-started#fallback)
.

PythonJS/TS

    # Get current production prompt version and cache for 5 minutes
    prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=300)
     
    # Disable caching for a prompt
    prompt = langfuse.get_prompt("movie-critic", cache_ttl_seconds=0)

### Performance measurement (excluding cache hits)[](#performance-measurement-excluding-cache-hits)

We measured the execution time of the following snippet. We disabled the cache to measure the performance of the prompt retrieval and compilation.

    prompt = langfuse.get_prompt("perf-test", cache_ttl_seconds=0) # disable cache
    prompt.compile(input="test")

Results from 1000 sequential executions in a local jupyter notebook using Langfuse Cloud (includes network latency):

![Performance Chart](https://langfuse.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fprompt-performance-chart.3c3545c4.png&w=1200&q=75)

    count  1000.000000
    mean      0.178465 sec
    std       0.058125 sec
    min       0.137314 sec
    25%       0.161333 sec
    50%       0.165919 sec
    75%       0.171736 sec
    max       0.687994 sec

Optional: Fallback for guaranteed availability[](#fallback)

------------------------------------------------------------

The Langfuse API has high uptime and prompts are cached locally in the SDKs to prevent network issues from affecting your application.

However, `get_prompt()`/`getPrompt()` will throw an exception if:

1.  No local (fresh or stale) cached prompt is available -> new application instance
2.  _and_ network request fails -> networking or Langfuse API issue

To prevent this, you can optionally provide a `fallback` prompt that will be used in these cases. This is especially useful for critical applications where prompt availability is crucial.

PythonJS/TS

    from langfuse import Langfuse
    langfuse = Langfuse()
     
    # Get `text` prompt with fallback
    prompt = langfuse.get_prompt(
      "movie-critic",
      fallback="Do you like {{movie}}?"
    )
     
    # Get `chat` prompt with fallback
    chat_prompt = langfuse.get_prompt(
      "movie-critic-chat",
      type="chat",
      fallback=[{"role": "system", "content": "You are an expert on {{movie}}"}]
    )
     
    # True if the prompt is a fallback
    prompt.is_fallback

FAQ[](#faq)

------------

*   [How do I link prompt management with tracing in Langfuse to see which prompt versions were used?](/faq/all/link-prompt-management-with-tracing)
    

[Query Traces](/docs/query-traces "Query Traces")
[Example OpenAI Functions](/docs/prompts/example-openai-functions "Example OpenAI Functions")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#62111712120d1016220e030c05041711074c010d0f)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.