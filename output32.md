---
domain: langfuse.com
path: /docs/integrations/openai/js/get-started
url: https://langfuse.com/docs/integrations/openai/js/get-started
---

[Join us in Engineering & DevRel â†’We're hiring. Join us in Product Eng, Backend Eng, and DevRel â†’](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference â†— (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference â†— (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API â†— (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK â†— (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK â†— (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support â†— (opens in a new tab)](/support)
    

Light

On This Page

*   [How it works](#how-it-works)
    
*   [Install Langfuse SDK](#install-langfuse-sdk)
    
*   [Call OpenAI methods with the wrapped client](#call-openai-methods-with-the-wrapped-client)
    
*   [Troubleshooting](#troubleshooting)
    
*   [Queuing and batching of events](#queuing-and-batching-of-events)
    
*   [Advanced usage](#advanced-usage)
    
*   [Custom trace properties](#custom-trace-properties)
    
*   [Link to Langfuse prompts](#link-to-langfuse-prompts)
    
*   [Nested traces](#nested-traces)
    
*   [Example](#example)
    
*   [FAQ](#faq)
    

[Question? Give us feedback â†’ (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9COSS%20Observability%20for%20OpenAI%20SDK%20(JS%2FTS)%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/openai/js/get-started.mdx)
Scroll to top

Docs

Integrations

OpenAI SDK

JS/TS

Get Started

Observability for OpenAI SDK (JS/TS)
====================================

Looking for the Python version? [Check it out here](/docs/integrations/openai/python/get-started)
.

The Langfuse JS/TS SDK offers a wrapper function around the OpenAI SDK, enabling you to easily add observability to your OpenAI calls. This includes tracking latencies, time-to-first-token on stream responses, errors, and model usage.

    import OpenAI from "openai";
    import { observeOpenAI } from "langfuse";
     
    const openai = observeOpenAI(new OpenAI());
     
    const res = await openai.chat.completions.create({
      messages: [{ role: "system", content: "Tell me a story about a dog." }],
    });

Langfuse automatically tracks:

*   All prompts/completions with support for streaming and function calling
*   Total latencies and time-to-first-token
*   OpenAI API Errors
*   Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost)
    )

_In the Langfuse Console_

How it works[](#how-it-works)

------------------------------

### Install Langfuse SDK[](#install-langfuse-sdk)

The integration is compatible with OpenAI SDK versions `>=4.0.0`.

    npm install langfuse openai

### Call OpenAI methods with the wrapped client[](#call-openai-methods-with-the-wrapped-client)

Langfuse wraps the OpenAI SDK and provides the functionality and method signatures. You can call them as usual.

The `observeOpenAI` function automatically instantiates a Langfuse client in the background. You can either configure the client with environment variables or pass the configuration directly to the `observeOpenAI` function.

Environment variablesDirectly passed variables

Add your Langfuse credentials to your environment variables. You can find your credentials in your project settings in the Langfuse UI. Make sure that you have a `.env` file in your project root and a package like `dotenv` to load the variables.

.env

    LANGFUSE_SECRET_KEY="sk-lf-..."
    LANGFUSE_PUBLIC_KEY="pk-lf-..."
    LANGFUSE_BASEURL="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
    # LANGFUSE_BASEURL="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region

With your environment configured, call OpenAI SDK methods as usual from the wrapped client.

    import OpenAI from "openai";
    import { observeOpenAI } from "langfuse";
     
    const openai = observeOpenAI(new OpenAI());
     
    const res = await openai.chat.completions.create({
      messages: [{ role: "system", content: "Tell me a story about a dog." }],
      model: "gpt-3.5-turbo",
      max_tokens: 300,
    });

Done!âœ¨ You now have full observability of your OpenAI calls in Langfuse.

Check out the notebook for end-to-end examples of the integration:

[Example notebook](/docs/integrations/openai/js/examples)

Troubleshooting[](#troubleshooting)

------------------------------------

### Queuing and batching of events[](#queuing-and-batching-of-events)

The Langfuse SDKs queue and batches events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to flush Langfuse to ensure that all events are flushed before the application exits.

    await openai.flushAsync(); // method added by Langfuse wrapper
     
    // If you have previously passed a parent span or trace for nesting, use that client for the flush call
    await langfuse.flushAsync();

Learn more about queuing and batching of events [here](/docs/tracing)
.

Advanced usage[](#advanced-usage)

----------------------------------

### Custom trace properties[](#custom-trace-properties)

You can add the following properties to the `langfuseConfig` of the `observeOpenAI` function to use additional Langfuse features:

| Property | Description |
| --- | --- |
| `generationName` | Set `generationName` to identify a specific type of generation. |
| `langfusePrompt` | Pass a created or fetched Langfuse prompt to link it with the generations |
| `metadata` | Set `metadata` with additional information that you want to see in Langfuse. |
| `sessionId` | The current [session](/docs/tracing-features/sessions)<br>. |
| `userId` | The current [user\_id](/docs/tracing-features/users)<br>. |
| `version` | Track different versions in Langfuse analytics |
| `release` | Track different releases in Langfuse analytics |
| `tags` | Set [tags](/docs/tracing-features/tags)<br> to categorize and filter traces. |

Example:

    const res = await observeOpenAI(new OpenAI(), {
      generationName: "Traced generation",
      metadata: { someMetadataKey: "someValue" },
      sessionId: "session-id",
      userId: "user-id",
      tags: ["tag1", "tag2"],
      version: "0.0.1",
      release: "beta",
    }).chat.completions.create({
      messages: [{ role: "system", content: "Tell me a story about a dog." }],
      model: "gpt-3.5-turbo",
      max_tokens: 300,
    });

Adding custom properties requires you to wrap the OpenAI SDK with the `observeOpenAI` function and pass the properties as the second `langfuseConfig` argument. Since the Langfuse client here is a singleton and the same client is used for all calls, you do not need to worry about mistakingly having multiple clients running.

### Link to Langfuse prompts[](#link-to-langfuse-prompts)

With [Langfuse Prompt management](/docs/prompts/get-started)
 you can effectively manage and version your prompts. You can link your OpenAI generations to a prompt by passing the `langfusePrompt` property to the `observeOpenAI` function.

    import { observeOpenAI } from "langfuse";
    import OpenAI from "openai";
     
    const langfusePrompt = await langfuse.getPrompt("prompt-name"); // Fetch a previously created prompt
     
    const res = await observeOpenAI(new OpenAI(), {
      langfusePrompt,
    }).completions.create({
      prompt: langfusePrompt.prompt,
      model: "gpt-3.5-turbo-instruct",
      max_tokens: 300,
    });

Resulting generations are now linked to the prompt in Langfuse, allowing you to track prompt usage and performance.

### Nested traces[](#nested-traces)

[Langfuse Tracing](/docs/tracing)
 groups multiple observations (can be any LLM or non-LLM calls) into a single trace. This integration by default creates a single trace for each OpenAI call.

By passing an existing trace or span to the `observeOpenAI` function as the `parent`, you can:

*   add non-OpenAI related observations to the trace.
*   group multiple OpenAI calls into a single trace while customizing the trace.
*   exert more control over the trace structure.
*   leverage all Langfuse Tracing features.

New to Langfuse Tracing? Checkout this [introduction](/docs/tracing)
 to the basic concepts.

Use the [Langfuse JS/TS SDK](/docs/sdk/typescript/guide)
 to create traces or spans manually and add OpenAI calls to it.

#### Example[](#example)

**Desired trace structure**

    TRACE: capital-poem-generator
    |
    |-- SPAN: France
    |   |
    |   |-- GENERATION: get-capital
    |   |
    |   |-- GENERATION: generate-poem

**Implementation**

    import Langfuse, { observeOpenAI } from "langfuse";
     
    // Initialize SDKs
    const langfuse = new Langfuse();
    const openai = new OpenAI();
     
    // Create trace and add params
    const trace = langfuse.trace({ name: "capital-poem-generator" });
     
    // Create span
    const country = "France";
    const span = trace.span({ name: country });
     
    // Call OpenAI
    const capital = (
      await observeOpenAI(openai, {
        parent: span,
        generationName: "get-capital",
      }).chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [\
          { role: "system", content: "What is the capital of the country?" },\
          { role: "user", content: country },\
        ],
      })
    ).choices[0].message.content;
     
    const poem = (
      await observeOpenAI(openai, {
        parent: span,
        generationName: "generate-poem",
      }).chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [\
          {\
            role: "system",\
            content: "You are a poet. Create a poem about this city.",\
          },\
          { role: "user", content: capital },\
        ],
      })
    ).choices[0].message.content;
     
    // End span to get span-level latencies
    span.end();
     
    // Flush the Langfuse client belonging to the parent span
    await langfuse.flushAsync();

FAQ[](#faq)

------------

*   [How to trace the OpenAI Assistants API?](/faq/all/openai-assistant-api)
    

[Structured Outputs](/docs/integrations/openai/python/structured-outputs "Structured Outputs")
[Example Notebook](/docs/integrations/openai/js/examples "Example Notebook")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#fd8e888d8d928f89bd919c939a9b888e98d39e9290)
[Talk to sales](/schedule-demo)

### Subscribe to updates

GetÂ updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

Â© 2022-2024 Langfuse GmbH / Finto Technologies Inc.