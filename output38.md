---
domain: langfuse.com
path: /docs/integrations/instructor
url: https://langfuse.com/docs/integrations/instructor
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Setup](#setup)
    
*   [Get started](#get-started)
    
*   [Langfuse-Instructor integration with sychnronous OpenAI client](#langfuse-instructor-integration-with-sychnronous-openai-client)
    
*   [Langfuse-Instructor integration with asychnronous OpenAI client](#langfuse-instructor-integration-with-asychnronous-openai-client)
    
*   [Example](#example)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9COSS%20Observability%20for%20Instructor%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/instructor.md)
Scroll to top

Docs

Integrations

Instructor

This is a Jupyter notebook

[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/integration_instructor.ipynb)
[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/integration_instructor.ipynb)

Instructor Integration
======================

[Instructor (opens in a new tab)](https://python.useinstructor.com/)
 ([GitHub (opens in a new tab)](https://github.com/jxnl/instructor/)
) is a popular library to get structured LLM outputs.

> Instructor makes it easy to reliably get structured data like JSON from Large Language Models (LLMs) like GPT-3.5, GPT-4, GPT-4-Vision, including open source models like Mistral/Mixtral from Together, Anyscale, Ollama, and llama-cpp-python. By leveraging various modes like Function Calling, Tool Calling and even constrained sampling modes like JSON mode, JSON Schema; Instructor stands out for its simplicity, transparency, and user-centric design. Under the hood, Instructor leverages Pydantic to do the heavy lifting, and provides a simple, easy-to-use API on top of it by helping you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.

This is a cookbook with examples of the Langfuse Integration for Python.

Setup[](#setup)

----------------

    %pip install langfuse openai pydantic instructor --upgrade

Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment.

    import os
     
    # get keys for your project from https://cloud.langfuse.com
    os.environ["LANGFUSE_PUBLIC_KEY"] = ""
    os.environ["LANGFUSE_SECRET_KEY"] = ""
     
    # your openai key
    os.environ["OPENAI_API_KEY"] = ""

Get started[](#get-started)

----------------------------

It is easy to use instructor with Langfuse. We use the [Langfuse OpenAI intgeration (opens in a new tab)](https://langfuse.com/docs/integrations/openai)
 and simply patch the client with instructor. This works with both synchronous and asynchronous clients.

### Langfuse-Instructor integration with sychnronous OpenAI client[](#langfuse-instructor-integration-with-sychnronous-openai-client)

    import instructor
    from langfuse.openai import OpenAI
    from pydantic import BaseModel
     
    # Patch Langfuse wrapper of synchronous OpenAI client with instructor
    client = instructor.patch(OpenAI())
     
    class WeatherDetail(BaseModel):
        city: str
        temperature: int
     
    # Run synchronous OpenAI client
    weather_info = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=WeatherDetail,
        messages=[\
            {"role": "user", "content": "The weather in Paris is 18 degrees Celsius."},\
        ],
    )
     
    print(weather_info.model_dump_json(indent=2))
    """
    {
      "city": "Paris",
      "temperature": 18
    }
    """

### Langfuse-Instructor integration with asychnronous OpenAI client[](#langfuse-instructor-integration-with-asychnronous-openai-client)

    import instructor
    from langfuse.openai import AsyncOpenAI
    from pydantic import BaseModel
     
    # Patch Langfuse wrapper of synchronous OpenAI client with instructor
    client = instructor.apatch(AsyncOpenAI())
     
    class WeatherDetail(BaseModel):
        city: str
        temperature: int
     
    # Run asynchronous OpenAI client
    task = client.chat.completions.create(
        model="gpt-3.5-turbo",
        response_model=WeatherDetail,
        messages=[\
            {"role": "user", "content": "The weather in Paris is 18 degrees Celsius."},\
        ],
    )
     
    response = await task
    print(response.model_dump_json(indent=2))
    """
    {
      "city": "Paris",
      "temperature": 18
    }
    """

Example[](#example)

--------------------

In this example, we first classify customer feedback into categories like `PRAISE`, `SUGGESTION`, `BUG` and `QUESTION`, and further scores the relvance of each feedback to the business on a scale of 0.0 to 1.0. In this case, we use the asynchronous OpenAI client `AsyncOpenAI` to classify and evaluate the feedback.

    from typing import List, Tuple
    from enum import Enum
     
    import asyncio
    import instructor
     
    from langfuse import Langfuse
    from langfuse.openai import AsyncOpenAI
    from langfuse.decorators import langfuse_context, observe
     
    from pydantic import BaseModel, Field, field_validator
     
    # Initialize Langfuse wrapper of AsyncOpenAI client
    client = AsyncOpenAI()
     
    # Patch the client with Instructor
    client = instructor.patch(client, mode=instructor.Mode.TOOLS)
     
    # Initialize Langfuse (needed for scoring)
    langfuse = Langfuse()
     
    # Rate limit the number of requests
    sem = asyncio.Semaphore(5)
     
    # Define feedback categories
    class FeedbackType(Enum):
        PRAISE = "PRAISE"
        SUGGESTION = "SUGGESTION"
        BUG = "BUG"
        QUESTION = "QUESTION"
     
    # Model for feedback classification
    class FeedbackClassification(BaseModel):
        feedback_text: str = Field(...)
        classification: List[FeedbackType] = Field(description="Predicted categories for the feedback")
        relevance_score: float = Field(
            default=0.0,
            description="Score of the query evaluating its relevance to the business between 0.0 and 1.0"
        )
     
        # Make sure feedback type is list
        @field_validator("classification", mode="before")
        def validate_classification(cls, v):
            if not isinstance(v, list):
                v = [v]
            return v
     
    @observe() # Langfuse decorator to automatically log spans to Langfuse
    async def classify_feedback(feedback: str) -> Tuple[FeedbackClassification, float]:
        """
        Classify customer feedback into categories and evaluate relevance.
        """
        async with sem:  # simple rate limiting
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                response_model=FeedbackClassification,
                max_retries=2,
                messages=[\
                    {\
                        "role": "user",\
                        "content": f"Classify and score this feedback: {feedback}",\
                    },\
                ],
            )
     
            # Retrieve observation_id of current span
            observation_id = langfuse_context.get_current_observation_id()
     
            return feedback, response, observation_id
     
    def score_relevance(trace_id: str, observation_id: str, relevance_score: float):
        """
        Score the relevance of a feedback query in Langfuse given the observation_id.
        """
        langfuse.score(
            trace_id=trace_id,
            observation_id=observation_id,
            name="feedback-relevance",
            value=relevance_score
        )
     
    @observe() # Langfuse decorator to automatically log trace to Langfuse
    async def main(feedbacks: List[str]):
        tasks = [classify_feedback(feedback) for feedback in feedbacks]
        results = []
     
        for task in asyncio.as_completed(tasks):
            feedback, classification, observation_id = await task
            result = {
                "feedback": feedback,
                "classification": [c.value for c in classification.classification],
                "relevance_score": classification.relevance_score,
            }
            results.append(result)
     
            # Retrieve trace_id of current trace
            trace_id = langfuse_context.get_current_trace_id()
     
            # Score the relevance of the feedback in Langfuse
            score_relevance(trace_id, observation_id, classification.relevance_score)
     
        # Flush observations to Langfuse
        langfuse_context.flush()
        return results
     
    feedback_messages = [\
        "The chat bot on your website does not work.",\
        "Your customer service is exceptional!",\
        "Could you add more features to your app?",\
        "I have a question about my recent order.",\
    ]
     
    feedback_classifications = await main(feedback_messages)
     
    for classification in feedback_classifications:
        print(f"Feedback: {classification['feedback']}")
        print(f"Classification: {classification['classification']}")
        print(f"Relevance Score: {classification['relevance_score']}")
     
    """
    Feedback: I have a question about my recent order.
    Classification: ['QUESTION']
    Relevance Score: 0.0
    Feedback: Could you add more features to your app?
    Classification: ['SUGGESTION']
    Relevance Score: 0.0
    Feedback: The chat bot on your website does not work.
    Classification: ['BUG']
    Relevance Score: 0.9
    Feedback: Your customer service is exceptional!
    Classification: ['PRAISE']
    Relevance Score: 0.9
    """

![Instructor Trace in Langfuse](https://langfuse.com/images/docs/instructor-trace.png)

[Dify.AI](/docs/integrations/dify "Dify.AI")
[Tracing](/docs/integrations/mirascope/tracing "Tracing")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#35464045455a47417559545b52534046501b565a58)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.