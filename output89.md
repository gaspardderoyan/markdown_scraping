---
domain: langfuse.com
path: /docs/integrations/litellm/example-proxy-js
url: https://langfuse.com/docs/integrations/litellm/example-proxy-js
---

[Join us in Engineering & DevRel â†’We're hiring. Join us in Product Eng, Backend Eng, and DevRel â†’](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference â†— (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference â†— (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API â†— (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK â†— (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK â†— (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support â†— (opens in a new tab)](/support)
    

Light

On This Page

*   [Install dependencies](#install-dependencies)
    
*   [Setup environment](#setup-environment)
    
*   [Setup Lite LLM Proxy](#setup-lite-llm-proxy)
    
*   [Log single LLM Call via Langfuse OpenAI Wrapper](#log-single-llm-call-via-langfuse-openai-wrapper)
    
*   [Trace nested LLM Calls using Langfuse JS SDK](#trace-nested-llm-calls-using-langfuse-js-sdk)
    
*   [Learn more](#learn-more)
    

[Question? Give us feedback â†’ (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CCookbook%3A%20LiteLLM%20(Proxy)%20%2B%20Langfuse%20OpenAI%20Integration%20(JS%2FTS)%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/litellm/example-proxy-js.md)
Scroll to top

Docs

Integrations

LiteLLM

Example Proxy (JS/TS)

This is a Jupyter notebook

[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/js_integration_litellm_proxy.ipynb)
[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/js_integration_litellm_proxy.ipynb)

Cookbook: LiteLLM (Proxy) + Langfuse OpenAI Integration (JS/TS)
===============================================================

This notebook demonstrates how to use the following stack to experiment with 100+ LLMs from different providers without changing code:

*   [**LiteLLM Proxy** (opens in a new tab)](https://docs.litellm.ai/docs/)
     ([GitHub (opens in a new tab)](https://github.com/BerriAI/litellm)
    ): Standardizes 100+ model provider APIs on the OpenAI API schema.
*   **Langfuse OpenAI SDK Wrapper** ([JS/TS (opens in a new tab)](https://langfuse.com/docs/integrations/openai/js/get-started)
    ): Natively instruments calls to 100+ models via the OpenAI SDK.
*   **Langfuse**: OSS LLM Observability, full overview [here (opens in a new tab)](https://langfuse.com/docs)
    .

Let's get started!

Install dependencies[](#install-dependencies)

----------------------------------------------

_Note: This cookbook uses Deno.js, which requires different syntax for importing packages and setting environment variables._

    import { OpenAI } from "npm:openai@^4.0.0";
    import { observeOpenAI } from "npm:langfuse@^3.6.0";

Setup environment[](#setup-environment)

----------------------------------------

    // Set env variables, Deno-specific syntax
    Deno.env.set("OPENAI_API_KEY", "");
    Deno.env.set("LANGFUSE_PUBLIC_KEY", "");
    Deno.env.set("LANGFUSE_SECRET_KEY", "");
    Deno.env.set("LANGFUSE_HOST", "https://cloud.langfuse.com"); // ðŸ‡ªðŸ‡º EU region
    // Deno.env.set("LANGFUSE_HOST", "https://us.cloud.langfuse.com"); // ðŸ‡ºðŸ‡¸ US region

Setup Lite LLM Proxy[](#setup-lite-llm-proxy)

----------------------------------------------

In this example, we'll use GPT-3.5-turbo directly from OpenAI, and llama3 and mistral via the Ollama on our local machine.

**Steps**

1.  Create a `litellm_config.yaml` to configure which models are available ([docs (opens in a new tab)](https://litellm.vercel.app/docs/proxy/configs)
    ). We'll use gpt-3.5-turbo, and llama3 and mistral via Ollama in this example. Make sure to replace `<openai_key>` with your OpenAI API key.
    
        model_list:
          - model_name: gpt-3.5-turbo
            litellm_params:
              model: gpt-3.5-turbo
              api_key: <openai_key>
          - model_name: ollama/llama3
            litellm_params:
              model: ollama/llama3
          - model_name: ollama/mistral
            litellm_params:
              model: ollama/mistral
    
2.  Ensure that you installed Ollama and have pulled the llama3 (8b) and mistral (7b) models: `ollama pull llama3 && ollama pull mistral`
3.  Run the following cli command to start the proxy: `litellm --config litellm_config.yaml`

The Lite LLM Proxy should be now running on [http://0.0.0.0:4000 (opens in a new tab)](http://0.0.0.0:4000)

To verify the connection you can run `litellm --test`

Log single LLM Call via Langfuse OpenAI Wrapper[](#log-single-llm-call-via-langfuse-openai-wrapper)

----------------------------------------------------------------------------------------------------

The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse.

We wrap the client for each call separately in order to be able to pass a name.

For more details, please refer to our [documentation (opens in a new tab)](https://langfuse.com/docs/integrations/openai/js/get-started)
.

    const PROXY_URL = "http://0.0.0.0:4000";
    const client = observeOpenAI(new OpenAI({baseURL: PROXY_URL}));
     
    const systemPrompt = "You are a very accurate calculator. You output only the result of the calculation.";
     
    const gptCompletion = await client.chat.completions.create({
      model: "gpt-3.5-turbo", 
      messages: [\
        {role: "system", content: systemPrompt},\
        {role: "user", content: "1 + 1 = "}\
      ],
    });
    console.log(gptCompletion.choices[0].message.content);
     
    const llamaCompletion = await client.chat.completions.create({
      model: "ollama/llama3",
      messages: [\
        {role: "system", content: systemPrompt},\
        {role: "user", content: "3 + 3 = "}\
      ],
    }); 
    console.log(llamaCompletion.choices[0].message.content);
     
    // notebook only: await events being flushed to Langfuse
    await client.flushAsync();

Public trace links for the following examples:

*   [GPT-3.5-turbo (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/5084c45a-4e73-45f6-97b2-ad134abc6af1?observation=20073c4e-749a-4289-ad78-6b48f6e61093)
    
*   [llama3 (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/05e0d142-20be-4e67-9baf-feb0d18271e6?observation=5bb6d269-8f3d-4c6e-8464-5103cbee4ada)
    

Trace nested LLM Calls using Langfuse JS SDK[](#trace-nested-llm-calls-using-langfuse-js-sdk)

----------------------------------------------------------------------------------------------

To capture nested LLM calls, use `langfuse.trace` to create a parent trace and pass it to `observeOpenAI`. This allows you to group multiple generations into a single trace, providing a comprehensive view of the interactions. You can also add rich metadata to the trace, such as custom names, tags, and userid. For more details, refer to the [Langfuse JS/TS SDK documentation (opens in a new tab)](https://langfuse.com/docs/sdk/typescript/guide)
.

We'll use the trace to log a rap battle between GPT-3.5-turbo, llama3, and mistral.

    import { Langfuse } from "npm:langfuse";
     
    const langfuse = new Langfuse();
     
    async function rapBattle(topic: string) {
      const trace = langfuse.trace({name: "Rap Battle", input: topic});
      
      let messages = [\
        {role: "system", content: "You are a rap artist. Drop a fresh line."},\
        {role: "user", content: `Kick it off, today's topic is ${topic}, here's the mic...`}\
      ];
     
      const gptCompletion = await observeOpenAI(new OpenAI({baseURL: PROXY_URL}), {
          parent: trace, generationName: "rap-gpt-3.5-turbo"
      }).chat.completions.create({
        model: "gpt-3.5-turbo",
        messages,
      });
      const firstRap = gptCompletion.choices[0].message.content;
      messages.push({role: "assistant", content: firstRap});
      console.log("Rap 1:", firstRap);
     
      const llamaCompletion = await observeOpenAI(new OpenAI({baseURL: PROXY_URL}), {
          parent: trace, generationName: "rap-llama3"
      }).chat.completions.create({
        model: "ollama/llama3", 
        messages,
      });
      const secondRap = llamaCompletion.choices[0].message.content;
      messages.push({role: "assistant", content: secondRap});
      console.log("Rap 2:", secondRap);
     
      const mistralCompletion = await observeOpenAI(new OpenAI({baseURL: PROXY_URL}), {
          parent: trace, generationName: "rap-mistral"
      }).chat.completions.create({
        model: "ollama/mistral",
        messages,
      });
      const thirdRap = mistralCompletion.choices[0].message.content;
      messages.push({role: "assistant", content: thirdRap});
      console.log("Rap 3:", thirdRap);
     
      trace.update({output: messages})
      return messages;
    }
     
    await rapBattle("typography");
    await langfuse.flushAsync();

**Example Trace** ([public link (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f1eee836-994b-4476-9dd5-3e09662a68c4)
)

![Public Trace](https://langfuse.com/images/cookbook/integration_litellm_proxy_trace.gif)

Learn more[](#learn-more)

--------------------------

Check out the docs to learn more about all components of this stack:

*   [LiteLLM Proxy (opens in a new tab)](https://docs.litellm.ai/docs/)
    
*   [Langfuse OpenAI SDK Wrapper (opens in a new tab)](https://langfuse.com/docs/integrations/openai/js/get-started)
    
*   [Langfuse (opens in a new tab)](https://langfuse.com/docs)
    

If you do not want to capture traces via the OpenAI SDK Wrapper, you can also directly log requests from the LiteLLM Proxy to Langfuse. For more details, refer to the [LiteLLM Docs (opens in a new tab)](https://litellm.vercel.app/docs/proxy/logging#logging-proxy-inputoutput---langfuse)
.

[Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python "Example Proxy (Python)")
[Vercel AI SDK](/docs/integrations/vercel-ai-sdk "Vercel AI SDK")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#deadabaeaeb1acaa9eb2bfb0b9b8abadbbf0bdb1b3)
[Talk to sales](/schedule-demo)

### Subscribe to updates

GetÂ updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

Â© 2022-2024 Langfuse GmbH / Finto Technologies Inc.