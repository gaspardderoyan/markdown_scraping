---
domain: langfuse.com
path: /docs/integrations/llama-index
url: https://langfuse.com/docs/integrations/llama-index
---

[Join us in Engineering & DevRel â†’We're hiring. Join us in Product Eng, Backend Eng, and DevRel â†’](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference â†— (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference â†— (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API â†— (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK â†— (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK â†— (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support â†— (opens in a new tab)](/support)
    

Light

On This Page

*   [Add Langfuse to your LlamaIndex application](#add-langfuse-to-your-llamaindex-application)
    
*   [Additional configuration](#additional-configuration)
    
*   [Queuing and flushing](#queuing-and-flushing)
    
*   [Custom trace parameters](#set-trace-params)
    
*   [Interoperability with Langfuse SDK](#interoperability-with-langfuse-sdk)
    

[Question? Give us feedback â†’ (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9COSS%20Observability%20for%20LlamaIndex%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/llama-index/get-started.mdx)
Scroll to top

Docs

Integrations

LlamaIndex

Get Started

ðŸ¦™ LlamaIndex Integration
=========================

**LlamaIndex** ([GitHub (opens in a new tab)](https://github.com/run-llama/llama_index)
) is an advanced "data framework" tailored for augmenting Large Language Models (LLMs) with private data.

> It streamlines the integration of diverse data sources and formats (APIs, PDFs, docs, SQL, etc.) through versatile data connectors and structures data into indices and graphs for LLM compatibility. The platform offers a sophisticated retrieval/query interface for enriching LLM inputs with context-specific outputs. Designed for both beginners and experts, LlamaIndex provides a user-friendly high-level API for easy data ingestion and querying, alongside customizable lower-level APIs for detailed module adaptation.

Langfuse offers a simple integration for automatic capture of [traces](/docs/tracing)
 and metrics generated in LlamaIndex applications. **Any feedback?** Let us know on Discord or GitHub. This is a new integration, and we'd love to hear your thoughts.

Currently only Python is supported by this integration. If you are interested in an integration with **LlamaIndex.TS**, add your upvote/comments to [this issue (opens in a new tab)](https://github.com/orgs/langfuse/discussions/1291)
.

_Example LlamaIndex trace in Langfuse. See a full video demo [here](/guides/videos/llama-index)
._

Add Langfuse to your LlamaIndex application[](#add-langfuse-to-your-llamaindex-application)

--------------------------------------------------------------------------------------------

Make sure you have both `llama-index` and `langfuse` installed.

    pip install llama-index langfuse

At the root of your LlamaIndex application, register Langfuse's `LlamaIndexCallbackHandler` in the LlamaIndex `Settings.callback_manager`. When instantiating `LlamaIndexCallbackHandler`, make sure to configure it correctly with your Langfuse API keys and the Host URL.

Environment variablesConstructor arguments

.env

    LANGFUSE_SECRET_KEY="sk-lf-..."
    LANGFUSE_PUBLIC_KEY="pk-lf-..."
    LANGFUSE_HOST="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
    # LANGFUSE_HOST="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region

    from llama_index.core import Settings
    from llama_index.core.callbacks import CallbackManager
    from langfuse.llama_index import LlamaIndexCallbackHandler
     
    langfuse_callback_handler = LlamaIndexCallbackHandler()
    Settings.callback_manager = CallbackManager([langfuse_callback_handler])

âœ¨

Done! Traces and metrics from your LlamaIndex application are now automatically tracked in Langfuse. If you construct a new index or query an LLM with your documents in context, your traces and metrics are immediately visible in the Langfuse UI.

Check out the notebook for end-to-end examples of the integration:

[Example Notebook](/docs/integrations/llama-index/example-python)

Additional configuration[](#additional-configuration)

------------------------------------------------------

### Queuing and flushing[](#queuing-and-flushing)

The Langfuse SDKs queue and batches events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to flush Langfuse to ensure that all events are flushed before the application exits.

    langfuse_handler.flush()

Learn more about queuing and batching of events [here](/docs/tracing)
.

### Custom trace parameters[](#set-trace-params)

You can update trace parameters at any time to add additional context to a trace, such as a user ID, session ID, or tags. See the [Python SDK Trace documentation](/docs/sdk/python#traces)
 for more information. All _subsequent_ traces will include these set parameters.

| Property | Description |
| --- | --- |
| `name` | Identify a specific type of trace, e.g. a use case or functionality. |
| `metadata` | Additional information that you want to see in Langfuse. Can be any JSON. |
| `session_id` | The current [session](/docs/tracing-features/sessions)<br>. |
| `user_id` | The current [user\_id](/docs/tracing-features/users)<br>. |
| `tags` | [Tags](/docs/tracing-features/tags)<br> to categorize and filter traces. |
| `version` | The specified version to trace [experiments](/docs/experimentation)<br>. |
| `release` | The specified release to trace [experiments](/docs/experimentation)<br>. |
| `sample_rate` | [Sample rate](/docs/tracing-features/sampling)<br> for tracing. |

    from llama_index.core import Settings
    from llama_index.core.callbacks import CallbackManager
    from langfuse import langfuse
     
    # Instantiate a new LlamaIndexCallbackHandler and register it in the LlamaIndex Settings
    langfuse_handler = LlamaIndexCallbackHandler()
    Settings.callback_manager = CallbackManager([langfuse_handler])
     
    def my_func():
      # Set trace parameters before executing your LlamaIndex code
      langfuse_callback_handler.set_trace_params(
        user_id="user-123",
        session_id="session-abc",
        tags=["production"]
      )
     
      # Your LlamaIndex code, trace will include the set parameters

> Notes
> 
> *   The params will be applied to all traces and spans created after the `set_trace_params` call. You can unset them by calling e.g. `set_trace_params(user_id=None)`.
> *   If you run this in a Jupyter Notebook, you need to run `set_trace_params` in the same cell as your LlamaIndex code.
> *   When setting a root trace or span, this setting will have no effect as the root trace or span will be used. See next section for more information.

### Interoperability with Langfuse SDK[](#interoperability-with-langfuse-sdk)

The Langfuse Python SDK is fully interoperable with the LlamaIndex integration.

This is useful when your **LlamaIndex executions are part of a larger application** and you want to link all traces and spans together. This can also be useful when you'd like to **group multiple LlamaIndex executions** to be part of the same trace or span.

Python DecoratorLow-level SDK

When using the [Langfuse `@observe()` decorator](/docs/sdk/python/decorators)
, `langfuse_context.get_current_llama_index_handler()` exposes a callback handler scoped to the current trace context, in this case `llama_index_fn()`. Pass it to the LlamaIndex `Settings.callback_manager` to trace subsequent LlamaIndex executions.

    from langfuse.decorators import langfuse_context, observe
    from llama_index.core import Document, VectorStoreIndex
    from llama_index.core import Settings
    from llama_index.core.callbacks import CallbackManager
     
    @observe()
    def llama_index_fn(question: str):
        # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function
        langfuse_handler = langfuse_context.get_current_llama_index_handler()
        Settings.callback_manager = CallbackManager([langfuse_handler])
     
        # Run application
        index = VectorStoreIndex.from_documents([doc1,doc2])
        response = index.as_query_engine().query(question)
        return response

> Notes
> 
> *   The Llamaindex intergation will not make any changes to your provided root trace or span. If you want to add additional context or input/output to your root trace or span, you can do so via the Python SDK.
> *   This uses context vars and will work reliably when run in the same cell in Jupyter.

[Upgrade Paths](/docs/integrations/langchain/upgrade-paths "Upgrade Paths")
[Example (Python)](/docs/integrations/llama-index/example-python "Example (Python)")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#d3a0a6a3a3bca1a793bfb2bdb4b5a6a0b6fdb0bcbe)
[Talk to sales](/schedule-demo)

### Subscribe to updates

GetÂ updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

Â© 2022-2024 Langfuse GmbH / Finto Technologies Inc.