---
domain: langfuse.com
path: /docs/integrations/openai/python/get-started
url: https://langfuse.com/docs/integrations/openai/python/get-started
---

[Join us in Engineering & DevRel â†’We're hiring. Join us in Product Eng, Backend Eng, and DevRel â†’](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference â†— (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference â†— (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API â†— (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK â†— (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK â†— (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support â†— (opens in a new tab)](/support)
    

Light

On This Page

*   [How it works](#how-it-works)
    
*   [Install Langfuse SDK](#install-langfuse-sdk)
    
*   [Switch to Langfuse Wrapped OpenAI SDK](#switch-to-langfuse-wrapped-openai-sdk)
    
*   [Use OpenAI SDK as usual](#use-openai-sdk-as-usual)
    
*   [Troubleshooting](#troubleshooting)
    
*   [Queuing and batching of events](#queuing-and-batching-of-events)
    
*   [Debug mode](#debug-mode)
    
*   [Streaming function / tool calls](#streaming-function--tool-calls)
    
*   [Advanced usage](#advanced-usage)
    
*   [Custom trace properties](#custom-trace-properties)
    
*   [Use Traces](#use-traces)
    
*   [OpenAI Beta APIs](#openai-beta-apis)
    
*   [FAQ](#faq)
    

[Question? Give us feedback â†’ (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9COSS%20Observability%20for%20OpenAI%20SDK%20(Python)%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/openai/python/get-started.mdx)
Scroll to top

Docs

Integrations

OpenAI SDK

Python

Get Started

Observability for OpenAI SDK (Python)
=====================================

Looking for the JS/TS version? [Check it out here](/docs/integrations/openai/js/get-started)
.

If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import. This works with OpenAI and Azure OpenAI.

    - import openai
    + from langfuse.openai import openai
     
    Alternative imports:
    + from langfuse.openai import OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI

Langfuse automatically tracks:

*   All prompts/completions with support for streaming, async and functions
*   Latencies
*   API Errors ([example](/docs/integrations/openai/track-errors)
    )
*   Model usage (tokens) and cost (USD) ([learn more](/docs/model-usage-and-cost)
    )

_In the Langfuse Console_

How it works[](#how-it-works)

------------------------------

ðŸ’¡

This integration does not cover the OpenAI _Assistants API_. You can instrument the assistants api via the `observe` decorator as shown in this [notebook](/docs/integrations/openai/python/assistants-api)
.

### Install Langfuse SDK[](#install-langfuse-sdk)

The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`.

    pip install langfuse openai

### Switch to Langfuse Wrapped OpenAI SDK[](#switch-to-langfuse-wrapped-openai-sdk)

Environment variablesAttributes

Add Langfuse credentials to your environment variables

.env

    LANGFUSE_SECRET_KEY="sk-lf-..."
    LANGFUSE_PUBLIC_KEY="pk-lf-..."
    LANGFUSE_HOST="https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
    # LANGFUSE_HOST="https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region

Change import

    - import openai
    + from langfuse.openai import openai
     
    Alternative imports:
    + from langfuse.openai import OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI

Optional, checks the SDK connection with the server. Not recommended for production usage.

    openai.langfuse_auth_check()

### Use OpenAI SDK as usual[](#use-openai-sdk-as-usual)

_No changes required._

Check out the notebook for end-to-end examples of the integration:

[Example notebook](/docs/integrations/openai/python/examples)
[Error tracking example](/docs/integrations/openai/python/track-errors)

Troubleshooting[](#troubleshooting)

------------------------------------

### Queuing and batching of events[](#queuing-and-batching-of-events)

The Langfuse SDKs queue and batches events in the background to reduce the number of network requests and improve overall performance. In a long-running application, this works without any additional configuration.

If you are running a short-lived application, you need to flush Langfuse to ensure that all events are flushed before the application exits.

    openai.flush_langfuse()

Learn more about queuing and batching of events [here](/docs/tracing)
.

### Debug mode[](#debug-mode)

If you are having issues with the integration, you can enable debug mode to get more information about the requests and responses.

    openai.langfuse_debug=True

### Streaming function / tool calls[](#streaming-function--tool-calls)

The capture of input and output when streaming function / tool calls is currently not supported. Please upvote this feature request in the [GitHub discussion (opens in a new tab)](https://github.com/orgs/langfuse/discussions/2055)
 if you would like to see this supported going forward.

Advanced usage[](#advanced-usage)

----------------------------------

### Custom trace properties[](#custom-trace-properties)

You can add the following properties to the openai method, e.g. `openai.chat.completions.create()`, to use additional Langfuse features:

| Property | Description |
| --- | --- |
| `name` | Set `name` to identify a specific type of generation. |
| `metadata` | Set `metadata` with additional information that you want to see in Langfuse. |
| `session_id` | The current [session](/docs/tracing-features/sessions)<br>. |
| `user_id` | The current [user\_id](/docs/tracing-features/users)<br>. |
| `tags` | Set [tags](/docs/tracing-features/tags)<br> to categorize and filter traces. |
| `trace_id` | See "Interoperability with Langfuse Python SDK" (below) for more details. |
| `parent_observation_id` | See "Interoperability with Langfuse Python SDK" (below) for more details. |
| `sample_rate` | [Sample rate](/docs/tracing-features/sampling)<br> for tracing. |

Example:

    openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[\
          {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},\
          {"role": "user", "content": "1 + 1 = "}],
        name="test-chat",
        metadata={"someMetadataKey": "someValue"},
    )

### Use Traces[](#use-traces)

[Langfuse Tracing](/docs/tracing)
 groups multiple observations (can be any LLM or non-LLM call) into a single trace. This integration by default creates a single trace for each openai call.

*   Add non-OpenAI related observations to the trace.
*   Group multiple OpenAI calls into a single trace while customizing the trace.
*   Have more control over the trace structure.
*   Use all Langfuse Tracing features.

New to Langfuse Tracing? Checkout this [introduction](/docs/tracing)
 to the basic concepts.

You can use any of the following three options:

1.  [Python `@observe()` decorator](/docs/sdk/python/decorators)
    
2.  Set `trace_id` property, best if you have an existing id from your application.
3.  Use the [low-level SDK](/docs/sdk/python/low-level-sdk)
     to create traces manually and add OpenAI calls to it.

Python DecoratorSet trace\_idLow-level SDK

TraceTrace and nested spans

Desired trace structure:

    TRACE: capital_poem_generator(input="Bulgaria")
    |
    |-- GENERATION: get-capital
    |
    |-- GENERATION: generate-poem

Implementation:

    from langfuse.decorators import observe
    from langfuse.openai import openai
     
    @observe()
    def capital_poem_generator(country)
      capital = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[\
            {"role": "system", "content": "What is the capital of the country?"},\
            {"role": "user", "content": country}],
        name="get-capital",
      ).choices[0].message.content
     
      poem = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[\
            {"role": "system", "content": "You are a poet. Create a poem about this city."},\
            {"role": "user", "content": capital}],
        name="generate-poem",
      ).choices[0].message.content
      return poem
     
    capital_poem_generator("Bulgaria")

### OpenAI Beta APIs[](#openai-beta-apis)

Since OpenAI beta APIs are changing frequently across versions, we fully support only the stable APIs in the OpenAI SDK. If you are using a beta API, you can still use the Langfuse SDK by wrapping the OpenAI SDK manually with the `@observe()` [decorator](/docs/sdk/python/decorators)
.

For **structured output parsing**, please use the `response_format` argument to `openai.chat.completions.create()` instead of the Beta API. This will allow you to set Langfuse attributes and metadata.

If you rely on parsing Pydantic defintions for your `response_format`, you may leverage the `type_to_response_format_param` utility function from the OpenAI Python SDK to convert the Pydantic definition to a `response_format` dictionary. This is the same function the OpenAI Beta API uses to convert Pydantic definitions to `response_format` dictionaries.

    from langfuse.openai import openai
    from openai.lib._parsing._completions import type_to_response_format_param
    from pydantic import BaseModel
     
    class CalendarEvent(BaseModel):
      name: str
      date: str
      participants: list[str]
     
     
    completion = openai.chat.completions.create(
        model="gpt-4o-2024-08-06",
        messages=[\
            {"role": "system", "content": "Extract the event information."},\
            {\
                "role": "user",\
                "content": "Alice and Bob are going to a science fair on Friday.",\
            },\
        ],
        response_format=type_to_response_format_param(CalendarEvent),
    )
     
    print(completion)
     
    openai.flush_langfuse()

FAQ[](#faq)

------------

*   [How to trace the OpenAI Assistants API?](/faq/all/openai-assistant-api)
    

[Overview](/docs/integrations/overview "Overview")
[Track Errors](/docs/integrations/openai/python/track-errors "Track Errors")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#10636560607f6264507c717e77766563753e737f7d)
[Talk to sales](/schedule-demo)

### Subscribe to updates

GetÂ updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

Â© 2022-2024 Langfuse GmbH / Finto Technologies Inc.