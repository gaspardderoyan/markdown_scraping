---
domain: langfuse.com
path: /docs/integrations/litellm/example-proxy-python
url: https://langfuse.com/docs/integrations/litellm/example-proxy-python
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Install dependencies](#install-dependencies)
    
*   [Setup environment](#setup-environment)
    
*   [Setup Lite LLM Proxy](#setup-lite-llm-proxy)
    
*   [Log single LLM Call via Langfuse OpenAI Wrapper](#log-single-llm-call-via-langfuse-openai-wrapper)
    
*   [Trace nested LLM Calls via Langfuse OpenAI Wrapper and @observe decorator](#trace-nested-llm-calls-via-langfuse-openai-wrapper-and-observe-decorator)
    
*   [Learn more](#learn-more)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CCookbook%3A%20LiteLLM%20(Proxy)%20%2B%20Langfuse%20OpenAI%20Integration%20%2B%20%40observe%20Decorator%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/litellm/example-proxy-python.md)
Scroll to top

Docs

Integrations

LiteLLM

Example Proxy (Python)

This is a Jupyter notebook

[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/integration_litellm_proxy.ipynb)
[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/integration_litellm_proxy.ipynb)

Cookbook: LiteLLM (Proxy) + Langfuse OpenAI Integration + `@observe` Decorator
==============================================================================

We want to share a stack that's commonly used by the Langfuse community to quickly experiment with 100+ models from different providers without changing code. This stack includes:

*   [**LiteLLM Proxy** (opens in a new tab)](https://docs.litellm.ai/docs/)
     ([GitHub (opens in a new tab)](https://github.com/BerriAI/litellm)
    ) which standardizes 100+ model provider APIs on the OpenAI API schema. It removes the complexity of direct API calls by centralizing interactions with these APIs through a single endpoint. You can also self-host the LiteLLM Proxy as it is open-source.
*   **Langfuse OpenAI SDK Wrapper** ([Python (opens in a new tab)](https://langfuse.com/docs/integrations/openai/python/get-started)
    , [JS (opens in a new tab)](https://langfuse.com/docs/integrations/openai/js/get-started)
    ) to natively instrument calls to all these 100+ models via the OpenAI SDK. This automatically captures token counts, latencies, streaming response times (time to first token), api errors, and more.
*   **Langfuse**: OSS LLM Observability, full overview [here (opens in a new tab)](https://langfuse.com/docs)
    .

This cookbook is an end-to-end guide to set up and use this stack. As we'll use Python in this example, we will also use the `@observe` decorator to create nested traces. More on this below.

Let's dive right in!

Install dependencies[](#install-dependencies)

----------------------------------------------

    !pip install "litellm[proxy]" langfuse openai

Setup environment[](#setup-environment)

----------------------------------------

    import os
    from langfuse.openai import openai
     
    # Get keys for your project from the project settings page
    # https://cloud.langfuse.com
    os.environ["LANGFUSE_PUBLIC_KEY"] = ""
    os.environ["LANGFUSE_SECRET_KEY"] = ""
    os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # 🇪🇺 EU region
    # os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # 🇺🇸 US region
     
    # Your openai key
    os.environ["OPENAI_API_KEY"] = ""
     
    # Test connection to Langfuse, not recommended for production as it is blocking
    openai.langfuse_auth_check()

Setup Lite LLM Proxy[](#setup-lite-llm-proxy)

----------------------------------------------

In this example, we'll use GPT-3.5-turbo directly from OpenAI, and llama3 and mistral via the Ollama on our local machine.

**Steps**

1.  Create a `litellm_config.yaml` to configure which models are available ([docs (opens in a new tab)](https://litellm.vercel.app/docs/proxy/configs)
    ). We'll use gpt-3.5-turbo, and llama3 and mistral via Ollama in this example. Make sure to replace `<openai_key>` with your OpenAI API key.
    
        model_list:
          - model_name: gpt-3.5-turbo
            litellm_params:
              model: gpt-3.5-turbo
              api_key: <openai_key>
          - model_name: ollama/llama3
            litellm_params:
              model: ollama/llama3
          - model_name: ollama/mistral
            litellm_params:
              model: ollama/mistral
    
2.  Ensure that you installed Ollama and have pulled the llama3 (8b) and mistral (7b) models: `ollama pull llama3 && ollama pull mistral`
3.  Run the following cli command to start the proxy: `litellm --config litellm_config.yaml`

The Lite LLM Proxy should be now running on [http://0.0.0.0:4000 (opens in a new tab)](http://0.0.0.0:4000)

To verify the connection you can run `litellm --test`

Log single LLM Call via Langfuse OpenAI Wrapper[](#log-single-llm-call-via-langfuse-openai-wrapper)

----------------------------------------------------------------------------------------------------

The Langfuse SDK offers a wrapper function around the OpenAI SDK, automatically logging all OpenAI calls as generations to Langfuse.

For more details, please refer to our [documentation (opens in a new tab)](https://langfuse.com/docs/integrations/openai/python/get-started)
.

    from langfuse.openai import openai
     
    # Set PROXY_URL to the url of your lite_llm_proxy (by default: http://0.0.0.0:4000)
    PROXY_URL="http://0.0.0.0:4000"
     
    system_prompt = "You are a very accurate calculator. You output only the result of the calculation."
     
    # Configure the OpenAI client to use the LiteLLM proxy
    client = openai.OpenAI(base_url=PROXY_URL)
     
    gpt_completion = client.chat.completions.create(
      model="gpt-3.5-turbo",
      name="gpt-3.5", # optional name of the generation in langfuse
      messages=[\
          {"role": "system", "content": system_prompt},\
          {"role": "user", "content": "1 + 1 = "}],
    )
    print(gpt_completion.choices[0].message.content)
     
    llama_completion = client.chat.completions.create(
      model="ollama/llama3",
      name="llama3", # optional name of the generation in langfuse
      messages=[\
          {"role": "system", "content": system_prompt},\
          {"role": "user", "content": "3 + 3 = "}],
    )
    print(llama_completion.choices[0].message.content)

Public trace links for the following examples:

*   [GPT-3.5-turbo (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/a4e67d7d-d9cb-455b-9795-3ad41f39431e?observation=81006513-82b1-4ae4-bb98-7e1bc6c009a7)
    
*   [llama3 (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/22fdce4a-4d74-4af3-9746-7bafaa45247c?observation=b9b30b5d-7fbf-40b4-acd9-4fdc1776cc87)
    

Trace nested LLM Calls via Langfuse OpenAI Wrapper and `@observe` decorator[](#trace-nested-llm-calls-via-langfuse-openai-wrapper-and-observe-decorator)

---------------------------------------------------------------------------------------------------------------------------------------------------------

Via the Langfuse `@observe()` decorator we can automatically capture execution details of any python function such as inputs, outputs, timings, and more. The decorator simplifies achieving in-depth observability in your applications with minimal code, especially when non-LLM calls are involved for knowledge retrieval (RAG) or api calls (agents).

For more details on how to utilize this decorator and customize your tracing, refer to our [documentation (opens in a new tab)](https://langfuse.com/docs/sdk/python/decorators)
.

Let's have a look at a simple example which uses all three models we have set up in the LiteLLM Proxy:

    from langfuse.decorators import observe
    from langfuse.openai import openai
     
    @observe()
    def rap_battle(topic: str):
        client = openai.OpenAI(
            base_url=PROXY_URL,
        )
     
        messages = [\
            {"role": "system", "content": "You are a rap artist. Drop a fresh line."},\
            {"role": "user", "content": "Kick it off, today's topic is {topic}, here's the mic..."}\
        ]
     
        # First model (gpt-3.5-turbo) starts the rap
        gpt_completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            name="rap-gpt-3.5-turbo", # add custom name to Langfuse observation
            messages=messages,
        )
        first_rap = gpt_completion.choices[0].message.content
        messages.append({"role": "assistant", "content": first_rap})
        print("Rap 1:", first_rap)
     
        # Second model (ollama/llama3) responds
        llama_completion = client.chat.completions.create(
            model="ollama/llama3",
            name="rap-llama3",
            messages=messages,
        )
        second_rap = llama_completion.choices[0].message.content
        messages.append({"role": "assistant", "content": second_rap})
        print("Rap 2:", second_rap)
     
        # Third model (ollama/mistral) adds the final touch
        mistral_completion = client.chat.completions.create(
            model="ollama/mistral",
            name="rap-mistral",
            messages=messages,
        )
        third_rap = mistral_completion.choices[0].message.content
        messages.append({"role": "assistant", "content": third_rap})
        print("Rap 3:", third_rap)
        
        return messages
     
    # Call the function
    rap_battle("typography")

[Public trace (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7b1af1be-8096-474e-a2fc-081538ca333c)

![Public Trace](https://langfuse.com/images/cookbook/integration_litellm_proxy_trace.gif)

Learn more[](#learn-more)

--------------------------

Check out the docs to learn more about all components of this stack:

*   [LiteLLM Proxy (opens in a new tab)](https://docs.litellm.ai/docs/)
    
*   [Langfuse OpenAI SDK Wrapper (opens in a new tab)](https://langfuse.com/docs/integrations/openai/python/get-started)
    
*   [Langfuse @observe() decorator (opens in a new tab)](https://langfuse.com/docs/sdk/python/decorators)
    
*   [Langfuse (opens in a new tab)](https://langfuse.com/docs)
    

If you do not want to capture traces via the OpenAI SDK Wrapper, you can also directly log requests from the LiteLLM Proxy to Langfuse. For more details, refer to the [LiteLLM Docs (opens in a new tab)](https://litellm.vercel.app/docs/proxy/logging#logging-proxy-inputoutput---langfuse)
.

[Tracing](/docs/integrations/litellm/tracing "Tracing")
[Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js "Example Proxy (JS/TS)")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#8cfff9fcfce3fef8cce0ede2ebeaf9ffe9a2efe3e1)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.