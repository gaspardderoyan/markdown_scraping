---
domain: langfuse.com
path: /docs/tracing-features/
url: https://langfuse.com/docs/tracing-features/
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Why Use Tracing for an LLM Application?](#why-use-tracing-for-an-llm-application)
    
*   [Why Langfuse?](#why-langfuse)
    
*   [Introduction to Traces in Langfuse](#introduction-to-traces-in-langfuse)
    
*   [Get Started](#get-started)
    
*   [Advanced usage](#advanced-usage)
    
*   [Enable/disable tracing](#enabledisable-tracing)
    
*   [Event queuing/batching](#queuing-batching)
    
*   [Configuration](#configuration)
    
*   [Manual flushing](#manual-flushing)
    
*   [FAQ](#faq)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CLLM%20Application%20Tracing%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/tracing.mdx)
Scroll to top

Docs

Introduction

LLM Application Tracing
=======================

LLM applications use increasingly complex abstractions, such as chains, agents with tools, and advanced prompts. The nested traces in Langfuse help to understand what is happening and identify the root cause of problems.

_Example trace of our [public demo](/docs/demo)
_

Why Use Tracing for an LLM Application?[](#why-use-tracing-for-an-llm-application)

-----------------------------------------------------------------------------------

*   Capture the full context of the execution, including API calls, context, prompts, parallelism, and more
*   Track model usage and cost
*   Collect user feedback
*   Identify low-quality outputs
*   Build fine-tuning and testing datasets

Why Langfuse?[](#why-langfuse)

-------------------------------

*   Open-source
*   Low performance overhead
*   SDKs for Python and JavaScript
*   Integrated with popular frameworks: OpenAI SDK (Python), Langchain (Python, JS), LlamaIndex (Python)
*   Public API for custom integrations
*   Suite of tools for the whole LLM application development lifecycle

Introduction to Traces in Langfuse[](#introduction-to-traces-in-langfuse)

--------------------------------------------------------------------------

A trace in Langfuse consists of the following objects:

*   A `trace` typically represents a single request or operation. It contains the overall input and output of the function, as well as metadata about the request, such as the user, the session, and tags.
*   Each trace can contain multiple `observations` to log the individual steps of the execution.
    *   Observations are of different types:
        *   `Events` are the basic building blocks. They are used to track discrete events in a trace.
        *   `Spans` represent durations of units of work in a trace.
        *   `Generations` are spans used to log generations of AI models. They contain additional attributes about the model, the prompt, and the completion. For generations, [token usage and costs](/docs/model-usage-and-cost)
             are automatically calculated.
    *   Observations can be nested.

**Hierarchical structure of traces in Langfuse**

**Example trace in Langfuse UI**

![Trace in Langfuse UI](https://langfuse.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftracing-observation-tree-light.36b56608.png&w=1080&q=75)

**Example trace in Langfuse UI**

![Trace in Langfuse UI](https://langfuse.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftracing-observation-tree-dark.5a37d59d.png&w=1080&q=75)

Get Started[](#get-started)

----------------------------

Follow the quickstart to add Langfuse tracing to your LLM app.

[Quickstart→](/docs/get-started)
[Interactive demo→](/docs/demo)

Advanced usage[](#advanced-usage)

----------------------------------

You can extend the tracing capabilities of Langfuse by using the following features:

[Sessions→](/docs/tracing-features/sessions)
[Users→](/docs/tracing-features/users)
[Tags→](/docs/tracing-features/tags)
[Metadata→](/docs/tracing-features/metadata)
[Trace URLs→](/docs/tracing-features/url)
[Export traces→](/docs/query-traces)

Enable/disable tracing[](#enabledisable-tracing)

-------------------------------------------------

All Langfuse SDKs and integrations are designed to be non-intrusive. You can add Langfuse tracing to your application while being able to enable it only in specific environments.

By default, the Langfuse Tracing is enabled if an API key is set. You can manually disable tracing via the `enabled` flag. See the documentation for the specific SDK or integration for more details.

Event queuing/batching[](#queuing-batching)

--------------------------------------------

Langfuse's client SDKs and integrations are all designed to queue and batch requests in the background to optimize API calls and network time. Batches are determined by a combination of time and size (number of events and size of batch).

### Configuration[](#configuration)

All integrations have a sensible default configuration, but you can customise the batching behaviour to suit your needs.

| Option (Python) \[SDK constructor, Environment\] | Option (JS) | Description |
| --- | --- | --- |
| `flush_at`, `LANGFUSE_FLUSH_AT` | `flushAt` | The maximum number of events to batch up before sending. |
| `flush_interval`, `LANGFUSE_FLUSH_INTERVAL` (s) | `flushInterval` (ms) | The maximum time to wait before sending a batch. |

You can e.g. set `flushAt=1` to send every event immediately, or `flushInterval=1000` to send every second.

### Manual flushing[](#manual-flushing)

This is especially relevant for short-lived applications like serverless functions. If you do not flush the client, you may lose events.

If you want to send a batch immediately, you can call the `flush` method on the client. In case of network issues, flush will log an error and retry the batch, it will never throw an exception.

PythonJS/TSOpenAI SDK (Python)LangchainLangchain (JS)LlamaIndex

    # Decorator
    from langfuse.decorators import langfuse_context
    langfuse_context.flush()
     
    # low-level SDK
    langfuse.flush()

If you exit the application, use `shutdown` method to make sure all requests are flushed and pending requests are awaited before the process exits. On success of this function, no more events will be sent to Langfuse API.

    langfuse.shutdown()

FAQ[](#faq)

------------

*   [How do I link prompt management with tracing in Langfuse to see which prompt versions were used?](/faq/all/link-prompt-management-with-tracing)
    
*   [I have setup Langfuse, but I do not see any traces in the dashboard. How to solve this?](/faq/all/missing-traces)
    

[Self-host (docker)](/docs/deployment/self-host "Self-host (docker)")
[Quickstart](/docs/get-started "Quickstart")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#c3b0b6b3b3acb1b783afa2ada4a5b6b0a6eda0acae)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.