---
domain: langfuse.com
path: /docs/security/overview
url: https://langfuse.com/docs/security/overview
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [How does LLM Security work?](#how-does-llm-security-work)
    
*   [1\. Run-time security measures](#1-run-time-security-measures)
    
*   [2\. Monitoring and evaluation of security measures with Langfuse](#2-monitoring-and-evaluation-of-security-measures-with-langfuse)
    
*   [Example: Anonymizing Personally Identifiable Information (PII)](#example-anonymizing-personally-identifiable-information-pii)
    
*   [Install packages](#install-packages)
    
*   [Anonymize and deanonymize PII and trace with Langfuse](#anonymize-and-deanonymize-pii-and-trace-with-langfuse)
    
*   [Instrument LLM call](#instrument-llm-call)
    
*   [Execute the application](#execute-the-application)
    
*   [Inspect trace in Langfuse](#inspect-trace-in-langfuse)
    
*   [Learn more](#learn-more)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CMonitor%20LLM%20Security%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/security/overview.mdx)
Scroll to top

Docs

LLM Security

Overview

Monitor LLM Security
====================

There are a host of potential safety risks involved with LLM-based applications. These include prompt injection, leakage of personally identifiable information (PII), or harmful prompts. Langfuse can be used to monitor and protect against these security risks, and investigate incidents when they occur.

LLM Security Monitoring with Langfuse

How does LLM Security work?[](#how-does-llm-security-work)

-----------------------------------------------------------

LLM Security can be addressed with a combination of

*   LLM Security libraries for run-time security measures
*   Langfuse for the ex-post evaluation of the effectiveness of these measures

### 1\. Run-time security measures[](#1-run-time-security-measures)

There are several popular security libraries that can be used to mitigate security risks in LLM-based applications. These include: [LLM Guard (opens in a new tab)](https://llm-guard.com)
, [Prompt Armor (opens in a new tab)](https://promptarmor.com)
, [NeMo Guardrails (opens in a new tab)](https://github.com/NVIDIA/NeMo-Guardrails)
, [Microsoft Azure AI Content Safety (opens in a new tab)](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety)
, [Lakera (opens in a new tab)](https://www.lakera.ai)
. These libraries help with security measures in the following ways:

1.  Catching and blocking a potentially harmful or inappropriate prompt before sending to the model
2.  Redacting sensitive PII before being sending into the model and then un-redacting in the response
3.  Evaluating prompts and completions on toxicity, relevance, or sensitive material at run-time and blocking the response if necessary

_In the following examples below we use the open source library LLM Guard. All examples easily translate to other libraries._

### 2\. Monitoring and evaluation of security measures with Langfuse[](#2-monitoring-and-evaluation-of-security-measures-with-langfuse)

Use Langfuse [tracing](/docs/tracing)
 to gain visibility and confidence in each step of the security mechanism. These are common workflows:

1.  Manually inspect traces to investigate security issues.
2.  Monitor security scores over time in the Langfuse Dashboard.
3.  Validate security checks. You can use Langfuse [scores](/docs/scores)
     to evaluate the effectiveness of security tools. Integrating Langfuse into your team's workflow can help teams identify which security risks are most prevalent and build more robust tools around those specific issues. There are two main workflows to consider:
    *   [Annotations (in UI)](/docs/scores/annotation)
        . If you establish a baseline by annotating a share of production traces, you can compare the security scores returned by the security tools with these annotations.
    *   [Automated evaluations](/docs/scores/model-based-evals)
        . Langfuse's model-based evaluations will run asynchronously and can scan traces for things such as toxicity or sensitivity to flag potential risks and identify any gaps in your LLM security setup. Check out the docs to learn more about how to set up these evaluations.
4.  Track Latency. Some LLM security checks need to be awaited before the model can be called, others block the response to the user. Thus they quickly are an essential driver of overall latency of an LLM application. Langfuse can help disect the latencies of these checks within a trace to understand whether the checks are worth the wait.

Example: Anonymizing Personally Identifiable Information (PII)[](#example-anonymizing-personally-identifiable-information-pii)

-------------------------------------------------------------------------------------------------------------------------------

Exposing PII to LLMs can pose serious security and privacy risks, such as violating contractual obligations or regulatory compliance requirements, or mitigating the risks of data leakage or a data breach.

Personally Identifiable Information (PII) includes:

*   Credit card number
*   Full name
*   Phone number
*   Email address
*   Social Security number
*   IP Address

The example below shows a simple application that summarizes a given court transcript. For privacy reasons, the application wants to anonymize PII before the information is fed into the model, and then un-redact the response to produce a coherent summary.

To read more about other security risks, including prompt injection, banned topics, or malicious URLs, please visit the LLM Guard [documentation (opens in a new tab)](https://llm-guard.com/)
 or check out or Security [cookbook](/docs/security/example-python)
.

### Install packages[](#install-packages)

    pip install llm-guard langfuse openai

First, import the security packages and Langfuse tools.

    from llm_guard.input_scanners import Anonymize
    from llm_guard.input_scanners.anonymize_helpers import BERT_LARGE_NER_CONF
    from langfuse.openai import openai # OpenAI integration
    from langfuse.decorators import observe, langfuse_context
    from llm_guard.output_scanners import Deanonymize
    from llm_guard.vault import Vault

### Anonymize and deanonymize PII and trace with Langfuse[](#anonymize-and-deanonymize-pii-and-trace-with-langfuse)

We break up each step of the process into its own function so we can track each step separately in Langfuse.

By decorating the functions with `@observe()`, we can trace each step of the process and monitor the risk scores returned by the security tools. This allows us to see how well the security tools are working and whether they are catching the PII as expected.

    vault = Vault()
     
    @observe()
    def anonymize(input: str):
      scanner = Anonymize(vault, preamble="Insert before prompt", allowed_names=["John Doe"], hidden_names=["Test LLC"],
                        recognizer_conf=BERT_LARGE_NER_CONF, language="en")
      sanitized_prompt, is_valid, risk_score = scanner.scan(prompt)
      return sanitized_prompt
     
    @observe()
    def deanonymize(sanitized_prompt: str, answer: str):
      scanner = Deanonymize(vault)
      sanitized_model_output, is_valid, risk_score = scanner.scan(sanitized_prompt, answer)
     
      return sanitized_model_output

### Instrument LLM call[](#instrument-llm-call)

In this example, we use the native OpenAI SDK integration, to instrument the LLM call. Thereby, we can automatically collect token counts, model parameters, and the exact prompt that was sent to the model.

Note: Langfuse [natively integrates](/docs/integrations)
 with a number of frameworks (e.g. LlamaIndex, LangChain, Haystack, ...) and you can easily instrument any LLM via the [SDKs](/docs/sdk)
.

    @observe()
    def summarize_transcript(prompt: str):
      sanitized_prompt = anonymize(prompt)
     
      answer = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            max_tokens=100,
            messages=[\
              {"role": "system", "content": "Summarize the given court transcript."},\
              {"role": "user", "content": sanitized_prompt}\
            ],
        ).choices[0].message.content
     
      sanitized_model_output = deanonymize(sanitized_prompt, answer)
     
      return sanitized_model_output

### Execute the application[](#execute-the-application)

Run the function. In this example, we input a section of a court transcript. Applications that handle sensitive information will often need to use anonymize and deanonymize functionality to comply with data privacy policies such as HIPAA or GDPR.

    prompt = """
    Plaintiff, Jane Doe, by and through her attorneys, files this complaint
    against Defendant, Big Corporation, and alleges upon information and belief,
    except for those allegations pertaining to personal knowledge, that on or about
    July 15, 2023, at the Defendant's manufacturing facility located at 123 Industrial Way, Springfield, Illinois, Defendant negligently failed to maintain safe working conditions,
    leading to Plaintiff suffering severe and permanent injuries. As a direct and proximate
    result of Defendant's negligence, Plaintiff has endured significant physical pain, emotional distress, and financial hardship due to medical expenses and loss of income. Plaintiff seeks compensatory damages, punitive damages, and any other relief the Court deems just and proper.
    """
    summarize_transcript(prompt)

### Inspect trace in Langfuse[](#inspect-trace-in-langfuse)

In this trace ([public link (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/43213866-3038-4706-ae3a-d39e9df459a2)
), we can see how the name of the plaintiff is anonymized before being sent to the model, and then un-redacted in the response. We can now evaluate run evaluations in Langfuse to control for the effectiveness of these measures.

Learn more[](#learn-more)

--------------------------

Find more examples of LLM security monitoring in our cookbook.

[Coookbook: Observing LLM Security](/docs/security/example-python)

[Custom via SDKs/API](/docs/scores/custom "Custom via SDKs/API")
[Example Python](/docs/security/example-python "Example Python")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#f4878184849b8680b498959a9392818791da979b99)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.