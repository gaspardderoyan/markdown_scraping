---
domain: langfuse.com
path: /docs/integrations/openai/python/examples
url: https://langfuse.com/docs/integrations/openai/python/examples
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Setup](#setup)
    
*   [Examples](#examples)
    
*   [Chat completion](#chat-completion)
    
*   [Chat completion (streaming)](#chat-completion-streaming)
    
*   [Chat completion (async)](#chat-completion-async)
    
*   [Functions](#functions)
    
*   [AzureOpenAI](#azureopenai)
    
*   [Group multiple generations into a single trace](#group-multiple-generations-into-a-single-trace)
    
*   [Fully featured: Interoperability with Langfuse SDK](#fully-featured-interoperability-with-langfuse-sdk)
    
*   [Programmatically add scores](#programmatically-add-scores)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CCookbook%3A%20OpenAI%20Integration%20(Python)%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/integrations/openai/python/examples.md)
Scroll to top

Docs

Integrations

OpenAI SDK

Python

Example Notebook

This is a Jupyter notebook

[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/integration_openai_sdk.ipynb)
[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/integration_openai_sdk.ipynb)

Cookbook: OpenAI Integration (Python)
=====================================

This is a cookbook with examples of the Langfuse Integration for OpenAI (Python).

Follow the [integration guide (opens in a new tab)](https://langfuse.com/docs/integrations/openai/get-started)
 to add this integration to your OpenAI project.

Setup[](#setup)

----------------

The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`.

    %pip install langfuse openai --upgrade

    import os
     
    # get keys for your project from https://cloud.langfuse.com
    os.environ["LANGFUSE_PUBLIC_KEY"] = ""
    os.environ["LANGFUSE_SECRET_KEY"] = ""
     
    # your openai key
    os.environ["OPENAI_API_KEY"] = ""
     
    # Your host, defaults to https://cloud.langfuse.com
    # For US data region, set to "https://us.cloud.langfuse.com"
    # os.environ["LANGFUSE_HOST"] = "http://localhost:3000"

    # instead of: import openai
    from langfuse.openai import openai

    # For debugging, checks the SDK connection with the server. Do not use in production as it adds latency.
    openai.langfuse_auth_check()

Examples[](#examples)

----------------------

### Chat completion[](#chat-completion)

    completion = openai.chat.completions.create(
      name="test-chat",
      model="gpt-3.5-turbo",
      messages=[\
          {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},\
          {"role": "user", "content": "1 + 1 = "}],
      temperature=0,
      metadata={"someMetadataKey": "someValue"},
    )

### Chat completion (streaming)[](#chat-completion-streaming)

Simple example using the OpenAI streaming functionality.

    completion = openai.chat.completions.create(
      name="test-chat",
      model="gpt-3.5-turbo",
      messages=[\
          {"role": "system", "content": "You are a professional comedian."},\
          {"role": "user", "content": "Tell me a joke."}],
      temperature=0,
      metadata={"someMetadataKey": "someValue"},
      stream=True
    )
     
    for chunk in completion:
      print(chunk.choices[0].delta.content, end="")

### Chat completion (async)[](#chat-completion-async)

Simple example using the OpenAI async client. It takes the Langfuse configurations either from the environment variables or from the attributes on the `openai` module.

    from langfuse.openai import AsyncOpenAI
     
    async_client = AsyncOpenAI()

    completion = await async_client.chat.completions.create(
      name="test-chat",
      model="gpt-3.5-turbo",
      messages=[\
          {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},\
          {"role": "user", "content": "1 + 100 = "}],
      temperature=0,
      metadata={"someMetadataKey": "someValue"},
    )

Go to [https://cloud.langfuse.com (opens in a new tab)](https://cloud.langfuse.com)
 or your own instance to see your generation.

![Chat completion](https://langfuse.com/images/docs/openai-chat.png)

### Functions[](#functions)

Simple example using Pydantic to generate the function schema.

    %pip install pydantic --upgrade

    from typing import List
    from pydantic import BaseModel
     
    class StepByStepAIResponse(BaseModel):
        title: str
        steps: List[str]
    schema = StepByStepAIResponse.schema() # returns a dict like JSON schema

    import json
    response = openai.chat.completions.create(
        name="test-function",
        model="gpt-3.5-turbo-0613",
        messages=[\
           {"role": "user", "content": "Explain how to assemble a PC"}\
        ],
        functions=[\
            {\
              "name": "get_answer_for_user_query",\
              "description": "Get user answer in series of steps",\
              "parameters": StepByStepAIResponse.schema()\
            }\
        ],
        function_call={"name": "get_answer_for_user_query"}
    )
     
    output = json.loads(response.choices[0].message.function_call.arguments)

Go to [https://cloud.langfuse.com (opens in a new tab)](https://cloud.langfuse.com)
 or your own instance to see your generation.

![Function](https://langfuse.com/images/docs/openai-function.png)

AzureOpenAI[](#azureopenai)

----------------------------

The integration also works with the `AzureOpenAI` and `AsyncAzureOpenAI` classes.

    AZURE_OPENAI_KEY=""
    AZURE_ENDPOINT=""
    AZURE_DEPLOYMENT_NAME="cookbook-gpt-4o-mini" # example deployment name

    # instead of: from openai import AzureOpenAI
    from langfuse.openai import AzureOpenAI

    client = AzureOpenAI(
        api_key=AZURE_OPENAI_KEY,  
        api_version="2023-03-15-preview",
        azure_endpoint=AZURE_ENDPOINT
    )

    client.chat.completions.create(
      name="test-chat-azure-openai",
      model=AZURE_DEPLOYMENT_NAME, # deployment name
      messages=[\
          {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},\
          {"role": "user", "content": "1 + 1 = "}],
      temperature=0,
      metadata={"someMetadataKey": "someValue"},
    )

Example trace: [https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7ceb3ee3-0f2a-4f36-ad11-87ff636efd1e (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7ceb3ee3-0f2a-4f36-ad11-87ff636efd1e)

Group multiple generations into a single trace[](#group-multiple-generations-into-a-single-trace)

--------------------------------------------------------------------------------------------------

Many applications require more than one OpenAI call. The `@observe()` decorator allows to nest all LLM calls of a single API invocation into the same `trace` in Langfuse.

    from langfuse.openai import openai
    from langfuse.decorators import observe
     
    @observe() # decorator to automatically create trace and nest generations
    def main(country: str, user_id: str, **kwargs) -> str:
        # nested generation 1: use openai to get capital of country
        capital = openai.chat.completions.create(
          name="geography-teacher",
          model="gpt-3.5-turbo",
          messages=[\
              {"role": "system", "content": "You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked."},\
              {"role": "user", "content": country}],
          temperature=0,
        ).choices[0].message.content
     
        # nested generation 2: use openai to write poem on capital
        poem = openai.chat.completions.create(
          name="poet",
          model="gpt-3.5-turbo",
          messages=[\
              {"role": "system", "content": "You are a poet. Create a poem about a city."},\
              {"role": "user", "content": capital}],
          temperature=1,
          max_tokens=200,
        ).choices[0].message.content
     
        return poem
     
    # run main function and let Langfuse decorator do the rest
    print(main("Bulgaria", "admin"))

Go to [https://cloud.langfuse.com (opens in a new tab)](https://cloud.langfuse.com)
 or your own instance to see your trace.

![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)

Fully featured: Interoperability with Langfuse SDK[](#fully-featured-interoperability-with-langfuse-sdk)

---------------------------------------------------------------------------------------------------------

The `trace` is a core object in Langfuse and you can add rich metadata to it. See [Python SDK docs (opens in a new tab)](https://langfuse.com/docs/sdk/python#traces-1)
 for full documentation on this.

Some of the functionality enabled by custom traces:

*   custom name to identify a specific trace-type
*   user-level tracking
*   experiment tracking via versions and releases
*   custom metadata

    from langfuse.openai import openai
    from langfuse.decorators import langfuse_context, observe
     
    @observe() # decorator to automatically create trace and nest generations
    def main(country: str, user_id: str, **kwargs) -> str:
        # nested generation 1: use openai to get capital of country
        capital = openai.chat.completions.create(
          name="geography-teacher",
          model="gpt-3.5-turbo",
          messages=[\
              {"role": "system", "content": "You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked."},\
              {"role": "user", "content": country}],
          temperature=0,
        ).choices[0].message.content
     
        # nested generation 2: use openai to write poem on capital
        poem = openai.chat.completions.create(
          name="poet",
          model="gpt-3.5-turbo",
          messages=[\
              {"role": "system", "content": "You are a poet. Create a poem about a city."},\
              {"role": "user", "content": capital}],
          temperature=1,
          max_tokens=200,
        ).choices[0].message.content
     
        # rename trace and set attributes (e.g., medatata) as needed
        langfuse_context.update_current_trace(
            name="City poem generator",
            session_id="1234",
            user_id=user_id,
            tags=["tag1", "tag2"],
            public=True,
            metadata = {
            "env": "development",
            },
            release = "v0.0.21"
        )
     
        return poem
     
    # create random trace_id, could also use existing id from your application, e.g. conversation id
    trace_id = str(uuid4())
     
    # run main function, set your own id, and let Langfuse decorator do the rest
    print(main("Bulgaria", "admin", langfuse_observation_id=trace_id))

Programmatically add scores[](#programmatically-add-scores)

------------------------------------------------------------

You can add [scores (opens in a new tab)](https://langfuse.com/docs/scores)
 to the trace, to e.g. record user feedback or some programmatic evaluation. Scores are used throughout Langfuse to filter traces and on the dashboard. See the docs on scores for more details.

The score is associated to the trace using the `trace_id`.

    from langfuse import Langfuse
    from langfuse.decorators import langfuse_context, observe
     
    langfuse = Langfuse()
     
    @observe() # decorator to automatically create trace and nest generations
    def main():
        # get trace_id of current trace
        trace_id = langfuse_context.get_current_trace_id()
     
        # rest of your application ...
     
        return "res", trace_id
     
    # execute the main function to generate a trace
    _, trace_id = main()
     
    # Score the trace from outside the trace context
    langfuse.score(
        trace_id=trace_id,
        name="my-score-name",
        value=1
    )

[Track Errors](/docs/integrations/openai/python/track-errors "Track Errors")
[Assistants API](/docs/integrations/openai/python/assistants-api "Assistants API")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#8dfef8fdfde2fff9cde1ece3eaebf8fee8a3eee2e0)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.