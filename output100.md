---
domain: langfuse.com
path: /docs/sdk/typescript/example-vercel-ai
url: https://langfuse.com/docs/sdk/typescript/example-vercel-ai
---

[Join us in Engineering & DevRel →We're hiring. Join us in Product Eng, Backend Eng, and DevRel →](/careers)

[![Langfuse Logo](https://langfuse.com/langfuse_logo_white.svg)![Langfuse Logo](https://langfuse.com/langfuse_logo.svg)](/)
[DocsDocs](/docs)
[PricingPricing](/pricing)
[ChangelogChangelog](/changelog)
[BlogBlog](/blog)

Demo

[Discord](https://discord.langfuse.com)
[](https://x.com/langfuse)
[GitHub](https://github.com/langfuse/langfuse "GitHub Repository")
[Sign Up](https://cloud.langfuse.com)

*   Docs
    
    [Guides](/guides)
    [FAQ](/faq)
    
*   [Overview](/docs)
    
*   [Interactive Demo](/docs/demo)
    
*   Self-host
    
    *   [Local (docker compose)](/docs/deployment/local)
        
    *   [Self-host (docker)](/docs/deployment/self-host)
        
    
*   Tracing
*   [Introduction](/docs/tracing)
    
*   [Quickstart](/docs/get-started)
    
*   Features
    
    *   [Sessions](/docs/tracing-features/sessions)
        
    *   [Users](/docs/tracing-features/users)
        
    *   [Metadata](/docs/tracing-features/metadata)
        
    *   [Tags](/docs/tracing-features/tags)
        
    *   [Trace URL](/docs/tracing-features/url)
        
    *   [Log Levels](/docs/tracing-features/log-levels)
        
    *   [Sampling](/docs/tracing-features/sampling)
        
    
*   SDKs
    
    *   [Overview](/docs/sdk/overview)
        
    *   Python
        
        *   [Decorators](/docs/sdk/python/decorators)
            
        *   [Example Notebook](/docs/sdk/python/example)
            
        *   [Low-level SDK](/docs/sdk/python/low-level-sdk)
            
        *   [Reference ↗ (opens in a new tab)](https://python.reference.langfuse.com)
            
        
    *   JS/TS
        
        *   [Guide](/docs/sdk/typescript/guide)
            
        *   [Guide (Web)](/docs/sdk/typescript/guide-web)
            
        *   [Example (Vercel AI)](/docs/sdk/typescript/example-vercel-ai)
            
        *   [Reference ↗ (opens in a new tab)](https://js.reference.langfuse.com)
            
        
    
*   Integrations
    
    *   [Overview](/docs/integrations/overview)
        
    *   OpenAI SDK
        
        *   Python
            
            *   [Get Started](/docs/integrations/openai/python/get-started)
                
            *   [Track Errors](/docs/integrations/openai/python/track-errors)
                
            *   [Example Notebook](/docs/integrations/openai/python/examples)
                
            *   [Assistants API](/docs/integrations/openai/python/assistants-api)
                
            *   [Structured Outputs](/docs/integrations/openai/python/structured-outputs)
                
            
        *   JS/TS
            
            *   [Get Started](/docs/integrations/openai/js/get-started)
                
            *   [Example Notebook](/docs/integrations/openai/js/examples)
                
            
        
    *   Langchain
        
        *   [Tracing](/docs/integrations/langchain/tracing)
            
        *   [Example Python](/docs/integrations/langchain/example-python)
            
        *   [Example JS](/docs/integrations/langchain/example-javascript)
            
        *   [Example LangGraph](/docs/integrations/langchain/example-python-langgraph)
            
        *   [Example LangServe](/docs/integrations/langchain/example-python-langserve)
            
        *   [Upgrade Paths](/docs/integrations/langchain/upgrade-paths)
            
        
    *   LlamaIndex
        
        *   [Get Started](/docs/integrations/llama-index/get-started)
            
        *   [Example (Python)](/docs/integrations/llama-index/example-python)
            
        
    *   Haystack
        
        *   [Get Started](/docs/integrations/haystack/get-started)
            
        *   [Example (Python)](/docs/integrations/haystack/example-python)
            
        
    *   LiteLLM
        
        *   [Tracing](/docs/integrations/litellm/tracing)
            
        *   [Example Proxy (Python)](/docs/integrations/litellm/example-proxy-python)
            
        *   [Example Proxy (JS/TS)](/docs/integrations/litellm/example-proxy-js)
            
        
    *   [Vercel AI SDK](/docs/integrations/vercel-ai-sdk)
        
    *   [Dify.AI](/docs/integrations/dify)
        
    *   [Instructor](/docs/integrations/instructor)
        
    *   Mirascope
        
        *   [Tracing](/docs/integrations/mirascope/tracing)
            
        *   [Example Notebook](/docs/integrations/mirascope/example-python)
            
        
    *   [Flowise](/docs/integrations/flowise)
        
    *   [Langflow](/docs/integrations/langflow)
        
    
*   [Query Traces](/docs/query-traces)
    
*   Develop
*   Prompt Management
    
    *   [Get Started](/docs/prompts/get-started)
        
    *   [Example OpenAI Functions](/docs/prompts/example-openai-functions)
        
    *   [Example Langchain (Py)](/docs/prompts/example-langchain)
        
    *   [Example Langchain (JS)](/docs/prompts/example-langchain-js)
        
    
*   [Playground](/docs/playground)
    
*   [Fine-tuning](/docs/fine-tuning)
    
*   Monitor
*   Analytics
    
    *   [Overview](/docs/analytics/overview)
        
    *   [PostHog Integration](/docs/analytics/posthog)
        
    *   [Daily Metrics API](/docs/analytics/daily-metrics-api)
        
    
*   [Model Usage & Cost](/docs/model-usage-and-cost)
    
*   Scores & Evaluation
    
    *   [Overview](/docs/scores/overview)
        
    *   [Annotation in UI](/docs/scores/annotation)
        
    *   [User Feedback](/docs/scores/user-feedback)
        
    *   [Model-based Evaluation](/docs/scores/model-based-evals)
        
    *   [Custom via SDKs/API](/docs/scores/custom)
        
    
*   LLM Security
    
    *   [Overview](/docs/security/overview)
        
    *   [Example Python](/docs/security/example-python)
        
    
*   Test
*   [Experimentation](/docs/experimentation)
    
*   Datasets
    
    *   [Overview](/docs/datasets/overview)
        
    *   [Cookbook](/docs/datasets/python-cookbook)
        
    
*   References
*   [API ↗ (opens in a new tab)](https://api.reference.langfuse.com)
    
*   [Python SDK ↗ (opens in a new tab)](https://python.reference.langfuse.com)
    
*   [JS SDK ↗ (opens in a new tab)](https://js.reference.langfuse.com)
    
*   More
*   [Access Control (RBAC)](/docs/rbac)
    
*   [Data Security & Privacy](/docs/data-security-privacy)
    
*   [Open Source](/docs/open-source)
    
*   [Roadmap](/docs/roadmap)
    
*   [Support ↗ (opens in a new tab)](/support)
    

Light

On This Page

*   [Backend API Route](#backend-api-route)
    
*   [Setup](#setup)
    
*   [Cookbook-only: Add prompt to Langfuse](#cookbook-only-add-prompt-to-langfuse)
    
*   [API handler](#api-handler)
    
*   [Our Frontend](#our-frontend)
    
*   [Explore the trace in the UI](#explore-the-trace-in-the-ui)
    
*   [Production Demo](#production-demo)
    

[Question? Give us feedback → (opens in a new tab)](https://github.com/langfuse/langfuse-docs/issues/new?title=Feedback%20for%20%E2%80%9CCookbook%3A%20Vercel%20AI%20SDK%20(JS%2FTS)%E2%80%9D&labels=feedback)
[Edit this page on GitHub](https://github.com/langfuse/langfuse-docs/tree/main/pages/docs/sdk/typescript/example-vercel-ai.md)
Scroll to top

Docs

SDKs

JS/TS

Example (Vercel AI)

Cookbook: Vercel AI SDK (JS/TS)
===============================

This is a cookbook with an end-to-end example on how to use [Langfuse Tracing (opens in a new tab)](https://langfuse.com/docs/tracing)
 together with the [Vercel AI SDK (opens in a new tab)](https://sdk.vercel.ai/docs)
.

Vercel AI SDK capabilities (from the docs)

> *   React Server Components API for streaming Generative UI
> *   SWR-powered React, Svelte, Vue and Solid helpers for streaming text responses and building chat and completion UIs
> *   First-class support for LangChain and OpenAI, Anthropic, Cohere, Hugging Face, Fireworks and Replicate
> *   Node.js, Serverless, and Edge Runtime support
> *   Callbacks for saving completed streaming responses to a database (in the same request)

In this end-to-end example, we use the [stream-lifecycle callbacks (opens in a new tab)](https://sdk.vercel.ai/docs/guides/providers/openai#guide-save-to-database-after-completion)
 to log all LLM calls to Langfuse via the [Langfuse TS SDK (opens in a new tab)](https://langfuse.com/docs/sdk/typescript/guide)
. It also supports Node.js, Serverless, and Edge Runtimes and intgrates well with Langchain JS ([integration docs (opens in a new tab)](https://langfuse.com/docs/integrations/langchain)
).

Hint: this is a deno-notebook and uses deno imports.

This notebook still uses legacy methods of the Vercel AI SDK, we will update it soon to use the new methods.

Backend API Route[](#backend-api-route)

----------------------------------------

### Setup[](#setup)

Initialize Langfuse with your API keys from the project settings in the Langfuse UI. As we will use OpenAI LLMs for this example, we also want to configure an OpenAI client.

    import { Langfuse } from "npm:langfuse"
    import { Configuration, OpenAIApi } from "npm:openai-edge";
     
    const langfuse = new Langfuse({
      publicKey: "",
      secretKey: "",
      baseUrl: "https://cloud.langfuse.com",
    });
     
    const openAIconfig = new Configuration({
      apiKey: "",
    });
    const openai = new OpenAIApi(openAIconfig);

### Cookbook-only: Add prompt to Langfuse[](#cookbook-only-add-prompt-to-langfuse)

We'll also use [Langfuse Prompt Management (opens in a new tab)](https://langfuse.com/docs/prompts)
 in this example. To be able to subsequently pull a production prompt from Langfuse, we'll quickly push one to Langfuse.

If you copy and paste this example, consider creating the prompt one-off or via the Langfuse UI before moving to prod. Alternatively, you can hardcode your prompts and avoid using Langfuse Prompt Management.

    await langfuse.createPrompt({
        name: "qa-prompt",
        prompt: "You are an extremely helpful assistant. Please assume that the person asking you questions needs your support and is totally honest with you.",
        isActive: true // immediately promote to production
    })

### API handler[](#api-handler)

The `ai` package provides a number of interfaces and abstractions.

In our example, we will use `OpenAIStream` to efficiently process and stream responses from OpenAI's models, and `StreamingTextResponse` to seamlessly deliver these AI-generated responses as HTTP streams to users.

    import { OpenAIStream, StreamingTextResponse } from "npm:ai";

Include the following if you deploy via Vercel:

    // select edge rutime for low latency
    export const runtime = 'edge';

Main API handler.

    /**
     * Example application simulating a QA chat bot using Langfuse and Vercel AI framework. 
     * 
     * Creates trace, retrieves a pre-saved prompt template, generates answer, and scores generation.
     * Utilizes OpenAI API for chat completion.
     *
     * @param {Request} req - The request object, containing session information, user ID, and messages.
     * @param {Response} res - The response object used to send back the processed data.
     * @returns {StreamingTextResponse} - A streamed response containing the output of the OpenAI model.
     */
    export default async function handler(req: Request, res: Response) {
        // initialize Langfuse 
        const trace = langfuse.trace({
            name: "QA",
            // Langfuse session tracking: https://langfuse.com/docs/tracing-features/sessions
            sessionId: req.sessionId, 
            // Langfuse user tracking: https://langfuse.com/docs/tracing-features/users
            userId: req.userId,
            // Make public, so everyone can view it via its URL (for this demo)
            public: true
        });
        
        // Format incoming messages for OpenAI API
        const messages = req.messages
        const openAiMessages = messages.map(({ content, role }) => ({
            content,
            role: role,
        }));
        
        // get last message
        const sanitizedQuery = messages[messages.length - 1].content.trim();
     
        trace.update({
            input: sanitizedQuery,
        });
        
        const promptName = req.promptName
        
        const promptSpan = trace.span({
            name: "fetch-prompt-from-langfuse",
            input: {
                promptName,
            },
        });
        
        // retrieve Langfuse prompt template with promptName
        const prompt = await langfuse.getPrompt(promptName);
        
        const promptTemplate = prompt.prompt
      
        promptSpan.end({
            output: { 
                promptTemplate,
            },
        });
        
        // merge prompt template and user input
        const assembledMessages = [\
            {\
                role: "system",\
                content: promptTemplate,\
            },\
            ...openAiMessages,\
        ];
          
        const generation = trace.generation({
            name: "generation",
            input: assembledMessages,
            model: "gpt-3.5-turbo",
            prompt, // link to prompt version from Langfuse prompt management
        });
     
        const response = await openai.createChatCompletion({
            model: "gpt-3.5-turbo",
            stream: true,
            messages: assembledMessages,
        });
        
        // Stream the response from OpenAI
        const stream = OpenAIStream(response, {
            onStart: () => {
                // Add completionStartTime timestamp to be able to break down latency
                // into delay until first token and the streaming duration
                generation.update({
                    completionStartTime: new Date(),
                });
            },
            onCompletion: async (completion) => {
                generation.end({
                    output: completion,
                    // Conditionally log a warning state
                    level: completion.includes("I don't know how to help with that")
                        ? "WARNING"
                        : "DEFAULT",
                    statusMessage: completion.includes("I don't know how to help with that")
                        ? "Refused to answer"
                        : undefined,
                });
                // Score generation, assume these answers are all correct
                if (!completion.includes("I don't know how to help with that")) {
                    generation.score({
                        name: "quality",
                        value: 1,
                        comment: "Factually correct",
                    });
                }
                trace.update({
                    output: completion,
                });
     
                // Make sure all events are successifully send to Langfuse before the stream terminates.
                await langfuse.flushAsync();
     
                // If you run on Vercel, waitUntil will do this in a non-blocking way
                // npm i @vercel/functions
                // import { waitUntil } from "@vercel/functions";
                // waitUntil(langfuse.flushAsync())
            },
        });
     
        // The AI package makes it super easy to add metadata as headers
        // It is a bit hacky, but we can e.g. pass the TraceURL to the frontend
        return new StreamingTextResponse(stream, {
            headers: {
                "X-Langfuse-Trace-Url": trace.getTraceUrl()
            },
        });
    }

Our Frontend[](#our-frontend)

------------------------------

If you use React, you'll probably want to use the `ai` package's React hooks for state management and to consume the streamed response. Learn more here: [https://sdk.vercel.ai/docs/getting-started#wire-up-a-ui (opens in a new tab)](https://sdk.vercel.ai/docs/getting-started#wire-up-a-ui)

To fit this into a notebook, we'll just call the API route directly.

    // sample request to test handler function
    const mockRequest = {
        "sessionId": "testSession",
        "userId": "testUser",
        "promptName": "qa-prompt",
        "messages": [\
            {\
                "role": "user",\
                "content": "What is love?"\
            },\
        ]
    }

    const response = await handler(mockRequest);
    const data = await response.text();
    console.log(data);

    Love is a complex and deep emotion that can manifest in various forms such as romantic love, platonic love, familial love, and love for oneself. It often involves feelings of care, affection, empathy, and a strong bond with another person. Love can bring joy, happiness, and fulfillment to our lives, but it can also be challenging and require effort, communication, and understanding to maintain healthy relationships. Overall, love is a fundamental aspect of human experience that can bring meaning and purpose to our lives.

### Explore the trace in the UI[](#explore-the-trace-in-the-ui)

Since we passed the trace url to the frontend via the http header and made it public, we cann access it on the Response object.

    console.log(response.headers.get("X-Langfuse-Trace-Url"))

    https://cloud.langfuse.com/trace/14cd44b6-1a56-46af-ba85-3fd91bbf9739

![Trace in Langfuse UI](https://langfuse.com/images/cookbook/js_tracing_example_vercel_ai_sdk_trace.png)

PS: As I ran this notebook a couple of times while putting it together, you can find a public [session (opens in a new tab)](https://langfuse.com/docs/tracing-features/sessions)
 view of me asking the same question `What is love?` [here (opens in a new tab)](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/sessions/testSession)
.

Production Demo[](#production-demo)

------------------------------------

We also use the Vercel AI SDK to power the public demo ([Docs Q&A Chatbot (opens in a new tab)](https://langfuse.com/demo)
). It's open source, see the full backend route here: [`qa-chatbot.ts` (opens in a new tab)](https://github.com/langfuse/langfuse-docs/blob/main/pages/api/qa-chatbot.ts)
.

[Guide (Web)](/docs/sdk/typescript/guide-web "Guide (Web)")
[Reference ↗](/docs/sdk/typescript/example-vercel-ai# "Reference ↗")

### Was this page useful?

YesCould be better

### Questions? We're here to help

[GitHub Q&AGitHub](/gh-support)
Chat [Email](/cdn-cgi/l/email-protection#1e6d6b6e6e716c6a5e727f7079786b6d7b307d7173)
[Talk to sales](/schedule-demo)

### Subscribe to updates

Get updates

Light

* * *

Platform

*   [LLM Tracing](/docs/tracing)
    
*   [Prompt Management](/docs/prompts/get-started)
    
*   [Evaluation](/docs/scores/overview)
    
*   [Manual Annotation](/docs/scores/annotation)
    
*   [Datasets](/docs/datasets/overview)
    
*   [Metrics](/docs/analytics)
    
*   [Playground](/docs/playground)
    

Integrations

*   [Python SDK](/docs/sdk/python)
    
*   [JS/TS SDK](/docs/sdk/typescript/guide)
    
*   [OpenAI SDK](/docs/integrations/openai/get-started)
    
*   [Langchain](/docs/integrations/langchain/tracing)
    
*   [Llama-Index](/docs/integrations/llama-index/get-started)
    
*   [Litellm](/docs/integrations/litellm)
    
*   [Dify](/docs/integrations/dify)
    
*   [Flowise](/docs/integrations/flowise)
    
*   [Langflow](/docs/integrations/langflow)
    
*   [Vercel AI SDK](/docs/sdk/typescript/example-vercel-ai)
    
*   [Instructor](/docs/integrations/instructor)
    
*   [Mirascope](/docs/integrations/mirascope)
    
*   [API](https://api.reference.langfuse.com/)
    

Resources

*   [Documentation](/docs)
    
*   [Interactive Demo](/demo)
    
*   [Video demo (3 min)](/video)
    
*   [Changelog](/changelog)
    
*   [Roadmap](/docs/roadmap)
    
*   [Pricing](/pricing)
    
*   [Enterprise](/enterprise)
    
*   [Self-hosting](/docs/deployment/self-host)
    
*   [Open Source](/docs/open-source)
    
*   [Why Langfuse?](/why)
    
*   [Status](https://status.langfuse.com)
    

About

*   [Blog](/blog)
    
*   [Careers](/careers)
    3
*   [About us](/about)
    
*   [Support](/support)
    
*   [Schedule Demo](/schedule-demo)
    
*   [OSS Friends](/oss-friends)
    

Legal

*   [Security](/security)
    
*   [Imprint](/imprint)
    
*   [Terms](/terms)
    
*   [Privacy](/privacy)
    

© 2022-2024 Langfuse GmbH / Finto Technologies Inc.